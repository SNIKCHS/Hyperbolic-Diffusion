{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def extract(v, t, x_shape):\n",
    "    \"\"\"\n",
    "    Extract some coefficients at specified timesteps, then reshape to\n",
    "    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
    "    \"\"\"\n",
    "    out = torch.gather(v, dim=0, index=t).float()\n",
    "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n",
    "\n",
    "class HyperbolicDiffusion(nn.Module):\n",
    "\n",
    "    def __init__(self ,T = 1000,beta_1=1e-4, beta_T=0.02):\n",
    "        super(HyperbolicDiffusion, self).__init__()\n",
    "\n",
    "        self.denoise_net = nn.Sequential(\n",
    "            nn.Linear(21,400),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(400,400),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(400,400),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(400,20),\n",
    "        )\n",
    "        self.T = T\n",
    "\n",
    "        self.register_buffer(\n",
    "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
    "\n",
    "        alphas = 1. - self.betas\n",
    "        self.register_buffer(\n",
    "            'sqrt_alphas', torch.sqrt(alphas))\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "        self.register_buffer(\n",
    "            'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
    "        self.register_buffer(\n",
    "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        loss = self.compute_loss(x)\n",
    "\n",
    "        return loss\n",
    "    def sample(self,h):\n",
    "        z_t = torch.randn_like(h)\n",
    "        for t in reversed(range(self.T)):\n",
    "            Time = torch.ones((h.size(0),),dtype=torch.int64, device=h.device) * t\n",
    "            noise = torch.randn_like(h)\n",
    "            pred_noise = self.denoise_net(torch.concat([Time[...,None],z_t],dim=1))\n",
    "            sqrt_one_minus_alphas_bar = extract(self.sqrt_one_minus_alphas_bar, Time, h.shape)\n",
    "            sqrt_alphas = extract(self.sqrt_alphas, Time, h.shape)\n",
    "            betas = extract(self.betas, Time, h.shape)\n",
    "            z_t = z_t-betas/sqrt_one_minus_alphas_bar * pred_noise\n",
    "            z_t = z_t/sqrt_alphas+betas*noise\n",
    "            print('t:',t,' z_t:',z_t[0])\n",
    "\n",
    "\n",
    "    def compute_loss(self, h):\n",
    "        t = torch.randint(self.T,size=(h.shape[0],), device=h.device)\n",
    "        noise = torch.randn_like(h)\n",
    "        x_t = (\n",
    "                extract(self.sqrt_alphas_bar, t, h.shape) * h +\n",
    "                extract(self.sqrt_one_minus_alphas_bar, t, h.shape) * noise)\n",
    "        # if(t[0]>950):\n",
    "        #     print(x_t[0])\n",
    "        t = t[...,None,None].repeat(1,h.size(1),1)\n",
    "\n",
    "        pred_noise = self.denoise_net(torch.concat([t,x_t],dim=2))\n",
    "        loss = F.mse_loss(pred_noise, noise, reduction='mean')\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/10000 [00:00<03:17, 50.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.0392, device='cuda:0', grad_fn=<MseLossBackward0>) [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1050/10000 [00:03<00:26, 340.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6562, device='cuda:0', grad_fn=<MseLossBackward0>) [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2069/10000 [00:06<00:21, 375.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0730, device='cuda:0', grad_fn=<MseLossBackward0>) [0.00045000000000000004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2756/10000 [00:08<00:22, 324.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(loss,lr_scheduler\u001B[38;5;241m.\u001B[39mget_last_lr())\n\u001B[1;32m---> 20\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     22\u001B[0m lr_scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\autograd\\__init__.py:166\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    162\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (inputs,) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m \\\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28mtuple\u001B[39m(inputs) \u001B[38;5;28;01mif\u001B[39;00m inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m()\n\u001B[0;32m    165\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[1;32m--> 166\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m \u001B[43m_make_grads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\autograd\\__init__.py:68\u001B[0m, in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     67\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 68\u001B[0m     new_grads\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreserve_format\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     70\u001B[0m     new_grads\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = HyperbolicDiffusion()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=2000,\n",
    "    gamma=float(0.9)\n",
    ")\n",
    "model = model.to('cuda')\n",
    "x = torch.ones((200,5,20),dtype=torch.float32,device='cuda')\n",
    "# x = torch.tensor([[1.5794, 2.5078, 0.0000, 0.0000, 1.7158, 1.0340, 0.0000, 1.3385, 0.0000,\n",
    "#         0.8585, 0.9584, 0.1076, 0.4893, 0.0000, 0.0000, 0.6972, 1.2082, 2.7626,\n",
    "#         0.0000, 0.0000]],device='cuda').repeat(200,1)\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(x)\n",
    "    if i%1000 ==0:\n",
    "        print(loss,lr_scheduler.get_last_lr())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.sample(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def extract(v, t, x_shape):\n",
    "#     \"\"\"\n",
    "#     Extract some coefficients at specified timesteps, then reshape to\n",
    "#     [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
    "#     \"\"\"\n",
    "#     out = torch.gather(v, dim=0, index=t).float()\n",
    "#     return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n",
    "#\n",
    "#\n",
    "# class HyperbolicDiffusion(nn.Module):\n",
    "#\n",
    "#     def __init__(self, T=1000, beta_1=1e-4, beta_T=0.02):\n",
    "#         super(HyperbolicDiffusion, self).__init__()\n",
    "#\n",
    "#         self.denoise_net = nn.Sequential(\n",
    "#             nn.Linear(31, 300),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(300, 300),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(300, 300),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(300, 30),\n",
    "#         )\n",
    "#         self.T = T\n",
    "#\n",
    "#         self.register_buffer(\n",
    "#             'betas', torch.linspace(beta_1, beta_T, T).double())\n",
    "#\n",
    "#         alphas = 1. - self.betas\n",
    "#         self.register_buffer(\n",
    "#             'sqrt_alphas', torch.sqrt(alphas))\n",
    "#         alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "#         self.register_buffer(\n",
    "#             'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
    "#         self.register_buffer(\n",
    "#             'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
    "#\n",
    "#     def forward(self):\n",
    "#         loss = self.compute_loss(torch.ones((20, 30), dtype=torch.float32))\n",
    "#\n",
    "#         return loss\n",
    "#\n",
    "#     def sample(self, h):\n",
    "#         z_t = torch.randn_like(h)\n",
    "#         for t in reversed(range(self.T)):\n",
    "#             Time = torch.ones((h.size(0),), dtype=torch.int64) * t\n",
    "#             noise = torch.randn_like(h)\n",
    "#             pred_noise = self.denoise_net(torch.concat([Time[..., None], z_t], dim=1))\n",
    "#             sqrt_one_minus_alphas_bar = extract(self.sqrt_one_minus_alphas_bar, Time, h.shape)\n",
    "#             sqrt_alphas = extract(self.sqrt_alphas, Time, h.shape)\n",
    "#             betas = extract(self.betas, Time, h.shape)\n",
    "#             z_t = z_t - betas / sqrt_one_minus_alphas_bar * pred_noise\n",
    "#             z_t = z_t / sqrt_alphas + betas * noise\n",
    "#             print('t:', t, ' z_t:', z_t[0])\n",
    "#\n",
    "#     def compute_loss(self, h):\n",
    "#         t = torch.randint(self.T, size=(h.shape[0],), device=h.device)\n",
    "#         noise = torch.randn_like(h)\n",
    "#         x_t = (\n",
    "#                 extract(self.sqrt_alphas_bar, t, h.shape) * h +\n",
    "#                 extract(self.sqrt_one_minus_alphas_bar, t, h.shape) * noise)\n",
    "#         t = t[..., None]\n",
    "#         pred_noise = self.denoise_net(torch.concat([t, x_t], dim=1))\n",
    "#         loss = F.mse_loss(pred_noise, noise, reduction='mean')\n",
    "#\n",
    "#         return loss\n",
    "#\n",
    "#\n",
    "# model = HyperbolicDiffusion()\n",
    "#\n",
    "# optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=2000,\n",
    "#     gamma=float(0.9)\n",
    "# )\n",
    "# for i in range(10000):\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = model().sum()\n",
    "#     print(loss, lr_scheduler.get_last_lr())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     lr_scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}