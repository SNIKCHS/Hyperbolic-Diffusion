{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0],\n         [1]],\n\n        [[0],\n         [1]],\n\n        [[0],\n         [1]]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = torch.arange(2)[None, :, None].expand(\n",
    "            3, -1, 1)\n",
    "ar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def extract(v, t, x_shape):\n",
    "    \"\"\"\n",
    "    Extract some coefficients at specified timesteps, then reshape to\n",
    "    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
    "    \"\"\"\n",
    "    out = torch.gather(v, dim=0, index=t).float()\n",
    "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, attn=False):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            # nn.LazyBatchNorm1d(),\n",
    "            nn.Linear(400, 400),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            # nn.LazyBatchNorm1d(),\n",
    "            nn.Linear(400, 400),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        if attn:\n",
    "            # self.attn = AttnBlock(out_ch)\n",
    "            pass\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.block1(x)\n",
    "        h = self.block2(h)\n",
    "        h = h + x\n",
    "        h = self.attn(h)\n",
    "        return h\n",
    "class HyperbolicDiffusion(nn.Module):\n",
    "\n",
    "    def __init__(self ,T = 1000,beta_1=1e-4, beta_T=0.02):\n",
    "        super(HyperbolicDiffusion, self).__init__()\n",
    "\n",
    "        self.denoise_net = nn.Sequential(\n",
    "            nn.Linear(21, 400),\n",
    "            ResBlock(),\n",
    "            ResBlock(),\n",
    "            ResBlock(),\n",
    "            ResBlock(),\n",
    "            nn.Linear(400, 400),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(400, 400),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(400, 20),\n",
    "        )\n",
    "        self.T = T\n",
    "\n",
    "        self.register_buffer(\n",
    "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
    "\n",
    "        alphas = 1. - self.betas\n",
    "        self.register_buffer(\n",
    "            'sqrt_alphas', torch.sqrt(alphas))\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "        self.register_buffer(\n",
    "            'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
    "        self.register_buffer(\n",
    "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        loss = self.compute_loss(x)\n",
    "\n",
    "        return loss\n",
    "    def sample(self,h):\n",
    "        z_t = torch.randn_like(h)\n",
    "        for t in reversed(range(self.T)):\n",
    "            Time = torch.ones((h.size(0),),dtype=torch.int64, device=h.device) * t\n",
    "            noise = torch.randn_like(h)\n",
    "            pred_noise = self.denoise_net(torch.concat([Time[...,None,None].repeat(1,h.size(1),1),z_t],dim=2))\n",
    "            sqrt_one_minus_alphas_bar = extract(self.sqrt_one_minus_alphas_bar, Time, h.shape)\n",
    "            sqrt_alphas = extract(self.sqrt_alphas, Time, h.shape)\n",
    "            betas = extract(self.betas, Time, h.shape)\n",
    "            z_t = z_t-betas/sqrt_one_minus_alphas_bar * pred_noise\n",
    "            z_t = z_t/sqrt_alphas+betas*noise\n",
    "            print('t:',t,' z_t:',z_t[0])\n",
    "\n",
    "\n",
    "    def compute_loss(self, h):\n",
    "        t = torch.randint(self.T,size=(h.shape[0],), device=h.device)\n",
    "        noise = torch.randn_like(h)\n",
    "        x_t = (\n",
    "                extract(self.sqrt_alphas_bar, t, h.shape) * h +\n",
    "                extract(self.sqrt_one_minus_alphas_bar, t, h.shape) * noise)\n",
    "        # if(t[0]>950):\n",
    "        #     print(x_t[0])\n",
    "        t = t[...,None,None].repeat(1,h.size(1),1)\n",
    "\n",
    "        pred_noise = self.denoise_net(torch.concat([t,x_t],dim=2))\n",
    "        loss = F.mse_loss(pred_noise, noise, reduction='mean')\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/10000 [00:00<05:08, 32.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.3208, grad_fn=<MseLossBackward0>) [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/10000 [00:00<05:34, 29.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(loss,lr_scheduler\u001B[38;5;241m.\u001B[39mget_last_lr())\n\u001B[1;32m---> 19\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     21\u001B[0m lr_scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = HyperbolicDiffusion()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=2000,\n",
    "    gamma=float(0.9)\n",
    ")\n",
    "x = torch.ones((200,1,20),dtype=torch.float32) *3\n",
    "# x = torch.tensor([[1.5794, 2.5078, 0.0000, 0.0000, 1.7158, 1.0340, 0.0000, 1.3385, 0.0000,\n",
    "#         0.8585, 0.9584, 0.1076, 0.4893, 0.0000, 0.0000, 0.6972, 1.2082, 2.7626,\n",
    "#         0.0000, 0.0000]],device='cuda').repeat(200,1)\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(x)\n",
    "    if i%1000 ==0:\n",
    "        print(loss,lr_scheduler.get_last_lr())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.sample(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def extract(v, t, x_shape):\n",
    "#     \"\"\"\n",
    "#     Extract some coefficients at specified timesteps, then reshape to\n",
    "#     [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
    "#     \"\"\"\n",
    "#     out = torch.gather(v, dim=0, index=t).float()\n",
    "#     return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n",
    "#\n",
    "#\n",
    "# class HyperbolicDiffusion(nn.Module):\n",
    "#\n",
    "#     def __init__(self, T=1000, beta_1=1e-4, beta_T=0.02):\n",
    "#         super(HyperbolicDiffusion, self).__init__()\n",
    "#\n",
    "#         self.denoise_net = nn.Sequential(\n",
    "#             nn.Linear(31, 300),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(300, 300),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(300, 300),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(300, 30),\n",
    "#         )\n",
    "#         self.T = T\n",
    "#\n",
    "#         self.register_buffer(\n",
    "#             'betas', torch.linspace(beta_1, beta_T, T).double())\n",
    "#\n",
    "#         alphas = 1. - self.betas\n",
    "#         self.register_buffer(\n",
    "#             'sqrt_alphas', torch.sqrt(alphas))\n",
    "#         alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "#         self.register_buffer(\n",
    "#             'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
    "#         self.register_buffer(\n",
    "#             'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
    "#\n",
    "#     def forward(self):\n",
    "#         loss = self.compute_loss(torch.ones((20, 30), dtype=torch.float32))\n",
    "#\n",
    "#         return loss\n",
    "#\n",
    "#     def sample(self, h):\n",
    "#         z_t = torch.randn_like(h)\n",
    "#         for t in reversed(range(self.T)):\n",
    "#             Time = torch.ones((h.size(0),), dtype=torch.int64) * t\n",
    "#             noise = torch.randn_like(h)\n",
    "#             pred_noise = self.denoise_net(torch.concat([Time[..., None], z_t], dim=1))\n",
    "#             sqrt_one_minus_alphas_bar = extract(self.sqrt_one_minus_alphas_bar, Time, h.shape)\n",
    "#             sqrt_alphas = extract(self.sqrt_alphas, Time, h.shape)\n",
    "#             betas = extract(self.betas, Time, h.shape)\n",
    "#             z_t = z_t - betas / sqrt_one_minus_alphas_bar * pred_noise\n",
    "#             z_t = z_t / sqrt_alphas + betas * noise\n",
    "#             print('t:', t, ' z_t:', z_t[0])\n",
    "#\n",
    "#     def compute_loss(self, h):\n",
    "#         t = torch.randint(self.T, size=(h.shape[0],), device=h.device)\n",
    "#         noise = torch.randn_like(h)\n",
    "#         x_t = (\n",
    "#                 extract(self.sqrt_alphas_bar, t, h.shape) * h +\n",
    "#                 extract(self.sqrt_one_minus_alphas_bar, t, h.shape) * noise)\n",
    "#         t = t[..., None]\n",
    "#         pred_noise = self.denoise_net(torch.concat([t, x_t], dim=1))\n",
    "#         loss = F.mse_loss(pred_noise, noise, reduction='mean')\n",
    "#\n",
    "#         return loss\n",
    "#\n",
    "#\n",
    "# model = HyperbolicDiffusion()\n",
    "#\n",
    "# optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=2000,\n",
    "#     gamma=float(0.9)\n",
    "# )\n",
    "# for i in range(10000):\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = model().sum()\n",
    "#     print(loss, lr_scheduler.get_last_lr())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     lr_scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}