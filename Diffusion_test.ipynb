{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack as spk\n",
    "import os\n",
    "from my_config import config_args\n",
    "from Model.HGDM import HyperbolicAE,HyperbolicDiffusion\n",
    "import optimizers\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from Model import Encoders, Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qm9data = QM9('./data/qm9.db', download=True,load_only=[QM9.U0])\n",
    "qm9split = './data/qm9split'\n",
    "print(len(qm9data))\n",
    "\n",
    "train, val, test = spk.train_test_split(\n",
    "        data=qm9data,\n",
    "        num_train=20000,\n",
    "        num_val=10000,\n",
    "        split_file=os.path.join(qm9split, \"split20000-10000.npz\"),\n",
    "    )\n",
    "print(len(train),len(val),len(test))\n",
    "\n",
    "train_loader = spk.AtomsLoader(train, batch_size=256, shuffle=False)\n",
    "val_loader = spk.AtomsLoader(val, batch_size=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "class obj(object):\n",
    "    def __init__(self, dict_):\n",
    "        self.__dict__.update(dict_)\n",
    "args = json.loads(json.dumps(config_args), object_hook=obj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_path = './saved_model/'+args.model+'-encoder_KL.pt'\n",
    "decoder_path = './saved_model/'+args.model+'-decoder_KL.pt'\n",
    "\n",
    "encoder = getattr(Encoders, args.model)(args)\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "if args.model == 'MLP':\n",
    "    decoder = Decoders.model2decoder[args.model](None, args)\n",
    "else:\n",
    "    decoder = Decoders.model2decoder[args.model](encoder.curvatures, args)\n",
    "decoder.load_state_dict(torch.load(decoder_path))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = HyperbolicDiffusion(args,encoder,decoder)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = getattr(optimizers, args.optimizer)(params=model.parameters(), lr=args.lr,\n",
    "                                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=args.lr_reduce_freq,\n",
    "    gamma=float(args.gamma)\n",
    ")\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    loss_sum,n = 0.0,0\n",
    "    for input in tqdm(train_loader):\n",
    "        for key in input:\n",
    "            input[key] = input[key].to(torch.device('cuda'))\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input).sum()\n",
    "        print(loss,lr_scheduler.get_last_lr())\n",
    "        loss.backward()\n",
    "        loss_sum += loss\n",
    "        n += 1\n",
    "        # if args.grad_clip is not None:\n",
    "        #     grad_clip = float(args.grad_clip)\n",
    "        #     all_params = list(model.parameters())\n",
    "        #     for param in all_params:\n",
    "        #         torch.nn.utils.clip_grad_value_(param, grad_clip)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "    if (epoch + 1) % args.log_freq == 0:\n",
    "        str = \" \".join(['Epoch: {:04d}'.format(epoch + 1),\n",
    "                               'lr: {}'.format(lr_scheduler.get_last_lr()[0]),\n",
    "                               'loss: {:.4f}'.format(loss_sum/n),\n",
    "                               'time: {:.4f}s'.format(time.time() - t)\n",
    "                               ])\n",
    "        print(str)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum,n = 0.0,0\n",
    "        for input in tqdm(val_loader):\n",
    "            for key in input:\n",
    "                input[key] = input[key].to(torch.device('cuda'))\n",
    "            t = time.time()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input).sum()\n",
    "            n += 1\n",
    "            loss_sum += loss\n",
    "\n",
    "        print('val_loss:',loss_sum.item()/n)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}