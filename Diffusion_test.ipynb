{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack as spk\n",
    "import os\n",
    "from my_config import config_args\n",
    "from Model.HGDM import HyperbolicAE,HyperbolicDiffusion\n",
    "import optimizers\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from Model import Encoders, Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133885\n",
      "30000 10000 93885\n"
     ]
    }
   ],
   "source": [
    "qm9data = QM9('./data/qm9.db', download=True,load_only=[QM9.U0])\n",
    "qm9split = './data/qm9split'\n",
    "print(len(qm9data))\n",
    "\n",
    "train, val, test = spk.train_test_split(\n",
    "        data=qm9data,\n",
    "        num_train=30000,\n",
    "        num_val=10000,\n",
    "        split_file=os.path.join(qm9split, \"split30000-10000.npz\"),\n",
    "    )\n",
    "print(len(train),len(val),len(test))\n",
    "\n",
    "train_loader = spk.AtomsLoader(train, batch_size=256, shuffle=False)\n",
    "val_loader = spk.AtomsLoader(val, batch_size=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 7112\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "class obj(object):\n",
    "    def __init__(self, dict_):\n",
    "        self.__dict__.update(dict_)\n",
    "args = json.loads(json.dumps(config_args), object_hook=obj)\n",
    "\n",
    "model = HyperbolicAE(args)\n",
    "\n",
    "optimizer = getattr(optimizers, args.optimizer)(params=model.parameters(), lr=args.lr,\n",
    "                                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=args.lr_reduce_freq,\n",
    "    gamma=float(args.gamma)\n",
    ")\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "device = torch.device('cuda')\n",
    "# Train model\n",
    "t_total = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_path = './saved_model/HNN-encoder.pt'\n",
    "decoder_path = './saved_model/HNN-decoder.pt'\n",
    "\n",
    "encoder = getattr(Encoders, args.model)(args)\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "decoder = Decoders.model2decoder[args.model](encoder.curvatures, args)\n",
    "decoder.load_state_dict(torch.load(decoder_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma schedule:\n",
      "[-5.         -4.693569   -4.3871384  -4.0810676  -3.7749963  -3.4689255\n",
      " -3.1628542  -2.8564234  -2.5499928  -2.2439218  -1.9378507  -1.6317797\n",
      " -1.3253489  -1.0192778  -0.71320677 -0.4074955  -0.10178375  0.20500612\n",
      "  0.5110779   0.8171487   1.12322     1.4296503   1.7353616   2.0421524\n",
      "  2.3482232   2.654294    2.9603653   3.2664366   3.572507    3.8785782\n",
      "  4.1846495   4.4907207   4.796792    5.1028633   5.4089346   5.715005\n",
      "  6.021076    6.3271475   6.6328583   6.9389296   7.24572     7.5514307\n",
      "  7.857502    8.163933    8.469285    8.775716    9.081428    9.387858\n",
      "  9.693929   10.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/118 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 12\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n\u001B[0;32m     14\u001B[0m n \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\GitHub repo\\Hyperbolic-Diffusion\\Model\\HGDM.py:241\u001B[0m, in \u001B[0;36mHyperbolicDiffusion.forward\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    237\u001B[0m adj \u001B[38;5;241m=\u001B[39m adj \u001B[38;5;241m*\u001B[39m adj\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (b,n_atom,n_atom)\u001B[39;00m\n\u001B[0;32m    240\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(positions,atomic_numbers, adj)\n\u001B[1;32m--> 241\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43matom_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt0_always\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;66;03m# output = self.decoder.decode(h, adj)\u001B[39;00m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;66;03m# atomic_numbers_one_hot = self.one_hot(atomic_numbers)\u001B[39;00m\n\u001B[0;32m    244\u001B[0m \u001B[38;5;66;03m# target = torch.concat([atomic_numbers_one_hot, positions.float()], dim=2)\u001B[39;00m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mD:\\GitHub repo\\Hyperbolic-Diffusion\\Model\\HGDM.py:435\u001B[0m, in \u001B[0;36mHyperbolicDiffusion.compute_loss\u001B[1;34m(self, h, node_mask, t0_always, adj)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;66;03m# Neural net prediction. 拟合噪声\u001B[39;00m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt:\u001B[39m\u001B[38;5;124m'\u001B[39m,t\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m--> 435\u001B[0m net_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdenoise_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;66;03m# Compute the error.\u001B[39;00m\n\u001B[0;32m    438\u001B[0m error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_error(net_out, gamma_t, eps)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\GitHub repo\\Hyperbolic-Diffusion\\Model\\HGDM.py:206\u001B[0m, in \u001B[0;36mDenoiseNet.forward\u001B[1;34m(self, t, z_t, adj)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, t, z_t, adj):\n\u001B[1;32m--> 206\u001B[0m     h_time \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty_like(z_t[:, \u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m.\u001B[39mfill_(\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    207\u001B[0m     z_t \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([z_t, h_time], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    208\u001B[0m     noise \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnet(z_t, adj)\n",
      "\u001B[1;31mValueError\u001B[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "model = HyperbolicDiffusion(args,encoder,decoder)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    loss_sum,n = 0.0,0\n",
    "    for input in tqdm(train_loader):\n",
    "        for key in input:\n",
    "            input[key] = input[key].to(torch.device('cuda'))\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input)\n",
    "        print(loss)\n",
    "        n += 1\n",
    "        loss_sum += loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}