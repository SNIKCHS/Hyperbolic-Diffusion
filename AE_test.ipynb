{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack as spk\n",
    "import os\n",
    "from my_config import config_args\n",
    "from Model.HGDM import HyperbolicAE,HyperbolicDiffusion\n",
    "import optimizers\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 获得训练数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133885\n"
     ]
    }
   ],
   "source": [
    "qm9data = QM9('./data/qm9.db', download=True,load_only=[QM9.U0])\n",
    "qm9split = './data/qm9split'\n",
    "print(len(qm9data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 93885\n"
     ]
    }
   ],
   "source": [
    "train, val, test = spk.train_test_split(\n",
    "        data=qm9data,\n",
    "        num_train=30000,\n",
    "        num_val=10000,\n",
    "        split_file=os.path.join(qm9split, \"split30000-10000.npz\"),\n",
    "    )\n",
    "print(len(train),len(val),len(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_loader = spk.AtomsLoader(train, batch_size=8, shuffle=False)\n",
    "val_loader = spk.AtomsLoader(val, batch_size=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class DenseAtt(nn.Module):\n",
    "    def __init__(self, in_features,edge_dim=1):\n",
    "        super(DenseAtt, self).__init__()\n",
    "        self.att_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * in_features + edge_dim, 2 * in_features, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(2 * in_features, in_features, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_features, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.in_features = in_features\n",
    "\n",
    "    def forward (self, x_left, x_right, distances, edge_mask):\n",
    "        distances = distances * edge_mask\n",
    "        x_cat = torch.concat((x_left, x_right,distances), dim=1)  # (b*n*n,2*dim+1)\n",
    "        att = self.att_mlp(x_cat)  # (b*n_node*n_node,1)\n",
    "\n",
    "        return att * edge_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "from schnetpack.nn import AtomDistances\n",
    "from schnetpack import Properties\n",
    "from manifolds import Hyperboloid\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            # nn.LazyBatchNorm1d(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            # nn.LazyBatchNorm1d(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.block1(x)\n",
    "        h = self.block2(h)\n",
    "        h = h + x\n",
    "        return h\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments, normalization_factor, aggregation_method: str):\n",
    "    \"\"\"Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n",
    "        Normalization: 'sum' or 'mean'.\n",
    "    \"\"\"\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    if aggregation_method == 'sum':\n",
    "        result = result / normalization_factor\n",
    "\n",
    "    if aggregation_method == 'mean':\n",
    "        norm = data.new_zeros(result.shape)\n",
    "        norm.scatter_add_(0, segment_ids, data.new_ones(data.shape))\n",
    "        norm[norm == 0] = 1\n",
    "        result = result / norm\n",
    "    return result\n",
    "class HGCLayer(nn.Module):\n",
    "    def __init__(self,manifold, in_features, out_features, c_in,c_out,act):\n",
    "        super().__init__()\n",
    "        self.manifold = manifold\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.c_in = c_in\n",
    "        self.bias = nn.Parameter(torch.Tensor(1, out_features))\n",
    "        self.linear = nn.Linear(in_features,out_features,bias=False)\n",
    "\n",
    "        self.normalization_factor = 100\n",
    "        self.aggregation_method = 'sum'\n",
    "        self.att = DenseAtt(out_features, edge_dim=1)\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_features, out_features))\n",
    "\n",
    "        self.c_out = c_out\n",
    "        self.act = act\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.xavier_uniform_(self.linear.weight, gain=0.1)\n",
    "        init.constant_(self.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        h, distances, edges, node_mask, edge_mask = input\n",
    "        h = self.HypLinear(h)\n",
    "        h = self.HypAgg(h, distances, edges, node_mask, edge_mask)\n",
    "        h = self.HypAct(h)\n",
    "        output = (h, distances, edges, node_mask, edge_mask)\n",
    "        return output\n",
    "\n",
    "    def HypLinear(self,x):\n",
    "        x = self.manifold.logmap0(x,self.c_in)\n",
    "        x = self.linear(x)\n",
    "        x = self.manifold.proj_tan0(x,self.c_in)\n",
    "        x = self.manifold.expmap0(x,self.c_in)\n",
    "\n",
    "        bias = self.manifold.proj_tan0(self.bias.view(1, -1), self.c_in)\n",
    "        hyp_bias = self.manifold.expmap0(bias, self.c_in)\n",
    "        res = self.manifold.mobius_add(x, hyp_bias, self.c_in)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def HypAgg(self,x, distances, edges, node_mask, edge_mask):\n",
    "        x_tangent = self.manifold.logmap0(x, c=self.c_in)  # (b*n_node,dim)\n",
    "        row, col = edges # 0,0,0...0,1 0,1,2..,0\n",
    "        x_tangent_row = x_tangent[row]\n",
    "        x_tangent_col = x_tangent[col]\n",
    "\n",
    "        x_local_tangent = self.manifold.logmap(x[row], x[col], c=self.c_in)  # (b*n_node*n_node,dim)  x_col落在x_row的切空间\n",
    "\n",
    "        att = self.att(x_tangent_row, x_tangent_col, distances,edge_mask)  # (b*n_node*n_node,dim)\n",
    "\n",
    "        agg = x_local_tangent * att\n",
    "\n",
    "        out = unsorted_segment_sum(agg, row, num_segments=x_tangent.size(0),  # num_segments=b*n_nodes\n",
    "                                   normalization_factor=self.normalization_factor,\n",
    "                                   aggregation_method=self.aggregation_method)  # sum掉第二个n_nodes (b*n_nodes*n_nodes,dim)->(b*n_nodes,dim)\n",
    "\n",
    "        # out = self.node_mlp(agg)\n",
    "        support_t = self.manifold.proj_tan(out, x, self.c_in)\n",
    "        output = self.manifold.expmap(support_t, x, c=self.c_in)\n",
    "        return output\n",
    "\n",
    "    def HypAct(self,x):\n",
    "        xt = self.act(self.manifold.logmap0(x, c=self.c_in))\n",
    "        xt = self.manifold.proj_tan0(xt, c=self.c_out)\n",
    "        out = self.manifold.expmap0(xt, c=self.c_out)\n",
    "        return out\n",
    "\n",
    "class HGCN(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.manifold = Hyperboloid()\n",
    "        self.act = nn.ReLU()\n",
    "        self.c = 1\n",
    "        self.Layer = nn.Sequential(\n",
    "            HGCLayer(self.manifold,20,256,self.c,self.c,self.act),\n",
    "            HGCLayer(self.manifold,256,256,self.c,self.c,self.act),\n",
    "            HGCLayer(self.manifold,256,256,self.c,self.c,self.act),\n",
    "            HGCLayer(self.manifold,256,256,self.c,self.c,self.act),\n",
    "        )\n",
    "        self.embedding = nn.Embedding(10,20)\n",
    "        self.distances = AtomDistances()\n",
    "        self._edges_dict = {}\n",
    "        self.out = nn.Sequential(\n",
    "            ResBlock(256),\n",
    "            nn.Linear(256,64),\n",
    "            ResBlock(64),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
    "    def forward(self,inputs):\n",
    "        atomic_numbers = inputs[Properties.Z]  # (b,n_atom)\n",
    "        positions = inputs[Properties.R]  # (b,n_atom,3)\n",
    "        positions -= positions.mean(dim=1, keepdim=True)\n",
    "        node_mask = inputs[Properties.atom_mask]  # (b,n_atom)\n",
    "        u0 = inputs['energy_U0']\n",
    "        batch_size,n_nodes = atomic_numbers.size()\n",
    "\n",
    "        size = node_mask.size()\n",
    "        edge_mask = node_mask.unsqueeze(2).expand(size[0], size[1], size[1])  # (b,n_atom,n_atom)\n",
    "        edge_mask = edge_mask * edge_mask.permute(0, 2, 1)\n",
    "\n",
    "        ar = torch.arange(atomic_numbers.size(1), device=atomic_numbers.device)[None, None, :].repeat(atomic_numbers.size(0),atomic_numbers.size(1),1)  # (b,n_atom,n_atom)\n",
    "        nbh = ar * edge_mask\n",
    "        h = self.embedding(atomic_numbers)  # (b,n_atom,embed)\n",
    "        distance = self.distances(positions,nbh.long(),neighbor_mask=edge_mask.bool())\n",
    "        edges = self.get_adj_matrix(n_nodes,batch_size)\n",
    "\n",
    "        h = h.view(batch_size * n_nodes, -1)\n",
    "        distance = distance.view(batch_size*n_nodes*n_nodes,1)\n",
    "        node_mask = node_mask.view(batch_size * n_nodes, -1)\n",
    "        edge_mask = edge_mask.view(batch_size * n_nodes * n_nodes, 1)\n",
    "\n",
    "        input = (h, distance, edges, node_mask, edge_mask)\n",
    "        output, distances, edges, node_mask, edge_mask = self.Layer(input)\n",
    "        output = self.out(output).squeeze()\n",
    "        output = output.view(batch_size,n_nodes).sum(1)\n",
    "        print(output,u0)\n",
    "        loss = self.loss_fn(output,u0)\n",
    "        return loss\n",
    "\n",
    "    def get_adj_matrix(self, n_nodes, batch_size):\n",
    "        # 对每个n_nodes，batch_size只要算一次\n",
    "        if n_nodes in self._edges_dict:\n",
    "            edges_dic_b = self._edges_dict[n_nodes]\n",
    "            if batch_size in edges_dic_b:\n",
    "                return edges_dic_b[batch_size]\n",
    "            else:\n",
    "                # get edges for a single sample\n",
    "                rows, cols = [], []\n",
    "                for batch_idx in range(batch_size):\n",
    "                    for i in range(n_nodes):\n",
    "                        for j in range(n_nodes):\n",
    "                            rows.append(i + batch_idx * n_nodes)\n",
    "                            cols.append(j + batch_idx * n_nodes)\n",
    "                edges = [torch.LongTensor(rows).to(self.device),\n",
    "                         torch.LongTensor(cols).to(self.device)]\n",
    "                edges_dic_b[batch_size] = edges\n",
    "                return edges\n",
    "        else:\n",
    "            self._edges_dict[n_nodes] = {}\n",
    "            return self.get_adj_matrix(n_nodes, batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "class obj(object):\n",
    "    def __init__(self, dict_):\n",
    "        self.__dict__.update(dict_)\n",
    "args = json.loads(json.dumps(config_args), object_hook=obj)\n",
    "device = torch.device('cpu')\n",
    "model = HGCN(device)\n",
    "optimizer = getattr(optimizers, args.optimizer)(params=model.parameters(), lr=1e-4,\n",
    "                                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=args.lr_reduce_freq,\n",
    "    gamma=float(args.gamma)\n",
    ")\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "logging.info(f\"Total number of parameters: {tot_params}\")\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1  loss: tensor(1.2939e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 2  loss: tensor(1.3081e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 3  loss: tensor(1.2689e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 4  loss: tensor(1.1791e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 5  loss: tensor(1.2046e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 6  loss: tensor(1.3027e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 7  loss: tensor(1.3315e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 8  loss: tensor(1.1820e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 9  loss: tensor(1.2028e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 10  loss: tensor(1.3567e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 11  loss: tensor(1.1278e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 12  loss: tensor(1.2705e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 13  loss: tensor(1.2205e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 14  loss: tensor(1.3299e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 15  loss: tensor(1.4327e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 16  loss: tensor(1.1657e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 17  loss: tensor(1.3892e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 18  loss: tensor(1.2733e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 19  loss: tensor(1.4946e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 20  loss: tensor(1.3124e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 21  loss: tensor(1.2562e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 22  loss: tensor(1.3410e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 23  loss: tensor(1.1592e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 24  loss: tensor(1.2267e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 25  loss: tensor(1.1234e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 26  loss: tensor(1.3090e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 27  loss: tensor(1.3024e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 28  loss: tensor(1.4432e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 29  loss: tensor(1.3572e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 30  loss: tensor(1.1457e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 31  loss: tensor(1.2472e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 32  loss: tensor(1.3036e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 33  loss: tensor(1.3464e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 34  loss: tensor(1.0776e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 35  loss: tensor(1.2494e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 36  loss: tensor(1.3433e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 37  loss: tensor(1.2591e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 38  loss: tensor(1.1723e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 39  loss: tensor(1.1986e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 40  loss: tensor(1.3351e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 41  loss: tensor(1.3440e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 42  loss: tensor(1.4205e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 43  loss: tensor(1.1955e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 44  loss: tensor(1.3061e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 45  loss: tensor(1.1547e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 46  loss: tensor(1.3101e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 47  loss: tensor(1.3660e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 48  loss: tensor(1.2221e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 49  loss: tensor(1.2771e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 50  loss: tensor(1.4122e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 51  loss: tensor(1.2789e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 52  loss: tensor(1.2160e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 53  loss: tensor(1.1878e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 54  loss: tensor(1.1970e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 55  loss: tensor(1.3304e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 56  loss: tensor(1.2292e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 57  loss: tensor(1.2117e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 58  loss: tensor(1.2697e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 59  loss: tensor(1.2372e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 60  loss: tensor(1.2964e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 61  loss: tensor(1.2832e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 62  loss: tensor(1.1402e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 63  loss: tensor(1.3592e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 64  loss: tensor(1.3895e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 65  loss: tensor(1.3188e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 66  loss: tensor(1.3101e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 67  loss: tensor(1.0533e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 68  loss: tensor(1.2710e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 69  loss: tensor(1.1839e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 70  loss: tensor(1.2673e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 71  loss: tensor(1.3442e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 72  loss: tensor(1.2626e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 73  loss: tensor(1.4004e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 74  loss: tensor(1.3001e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 75  loss: tensor(1.3179e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 76  loss: tensor(1.2215e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 77  loss: tensor(1.3042e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 78  loss: tensor(1.2238e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 79  loss: tensor(1.1173e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 80  loss: tensor(1.3075e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 81  loss: tensor(1.4033e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 82  loss: tensor(1.0927e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 83  loss: tensor(1.3850e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 84  loss: tensor(1.1949e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 85  loss: tensor(1.3247e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 86  loss: tensor(1.2146e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 87  loss: tensor(1.2401e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 88  loss: tensor(1.3551e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 89  loss: tensor(1.2908e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 90  loss: tensor(1.2951e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 91  loss: tensor(1.2615e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 92  loss: tensor(1.2469e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 93  loss: tensor(1.2475e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 94  loss: tensor(1.2087e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 95  loss: tensor(1.4703e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 96  loss: tensor(1.1651e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 97  loss: tensor(1.2425e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 98  loss: tensor(1.1917e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 99  loss: tensor(1.2016e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 100  loss: tensor(1.3756e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 101  loss: tensor(1.2747e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 102  loss: tensor(1.4509e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 103  loss: tensor(1.3175e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 104  loss: tensor(1.1652e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 105  loss: tensor(1.1022e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 106  loss: tensor(1.2387e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 107  loss: tensor(1.2415e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 108  loss: tensor(1.2631e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 109  loss: tensor(1.2941e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 110  loss: tensor(1.1325e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 111  loss: tensor(1.1512e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 112  loss: tensor(1.3475e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 113  loss: tensor(1.2487e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 114  loss: tensor(1.3466e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 115  loss: tensor(1.1918e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 116  loss: tensor(1.3029e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 117  loss: tensor(1.2559e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 118  loss: tensor(1.1934e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 119  loss: tensor(1.2775e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 120  loss: tensor(1.2257e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 121  loss: tensor(1.1826e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 122  loss: tensor(1.3352e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 123  loss: tensor(1.2882e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 124  loss: tensor(1.2608e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 125  loss: tensor(1.1979e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 126  loss: tensor(1.1579e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 127  loss: tensor(1.2211e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 128  loss: tensor(1.1056e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 129  loss: tensor(1.1691e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 130  loss: tensor(1.2248e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 131  loss: tensor(1.1565e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 132  loss: tensor(1.2042e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 133  loss: tensor(1.1215e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 134  loss: tensor(1.1529e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 135  loss: tensor(1.0184e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 136  loss: tensor(1.0767e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 137  loss: tensor(1.1233e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 138  loss: tensor(1.0311e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 139  loss: tensor(1.0632e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 140  loss: tensor(91900608., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 141  loss: tensor(65768872., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 142  loss: tensor(68025936., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 143  loss: tensor(57825048., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 144  loss: tensor(37789676., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 145  loss: tensor(53396120., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 146  loss: tensor(68844816., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 147  loss: tensor(33542884., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 148  loss: tensor(1.0441e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 149  loss: tensor(63500088., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 150  loss: tensor(76495448., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 151  loss: tensor(74567808., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 152  loss: tensor(94569800., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 153  loss: tensor(1.7741e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 154  loss: tensor(65623132., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 155  loss: tensor(82190864., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 156  loss: tensor(87851864., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 157  loss: tensor(63862840., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 158  loss: tensor(53202048., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 159  loss: tensor(61741768., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 160  loss: tensor(56893320., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 161  loss: tensor(47257588., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 162  loss: tensor(52800940., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 163  loss: tensor(70588160., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 164  loss: tensor(66512696., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 165  loss: tensor(52878184., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 166  loss: tensor(75917872., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 167  loss: tensor(62130824., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 168  loss: tensor(65951020., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 169  loss: tensor(42368064., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 170  loss: tensor(57279520., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 171  loss: tensor(36930948., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 172  loss: tensor(61085784., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 173  loss: tensor(50229984., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 174  loss: tensor(40581428., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 175  loss: tensor(54094968., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 176  loss: tensor(45275504., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 177  loss: tensor(49721248., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 178  loss: tensor(79229816., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 179  loss: tensor(8993083., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 180  loss: tensor(69041152., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 181  loss: tensor(58100300., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 182  loss: tensor(71837728., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 183  loss: tensor(48231672., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 184  loss: tensor(46086628., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 185  loss: tensor(92426976., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 186  loss: tensor(1.0155e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 187  loss: tensor(21548876., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 188  loss: tensor(64738192., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 189  loss: tensor(53241352., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 190  loss: tensor(75039944., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 191  loss: tensor(57052712., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 192  loss: tensor(55661384., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 193  loss: tensor(35754348., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 194  loss: tensor(50749964., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 195  loss: tensor(60856876., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 196  loss: tensor(55744252., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 197  loss: tensor(51962328., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 198  loss: tensor(67126104., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 199  loss: tensor(24693384., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 200  loss: tensor(38702764., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 201  loss: tensor(54332080., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 202  loss: tensor(58067784., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 203  loss: tensor(27776668., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 204  loss: tensor(46174600., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 205  loss: tensor(78236864., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 206  loss: tensor(48938572., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 207  loss: tensor(53249880., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 208  loss: tensor(67130080., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 209  loss: tensor(69814072., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 210  loss: tensor(51086532., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 211  loss: tensor(74386664., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 212  loss: tensor(74290976., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 213  loss: tensor(36180236., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 214  loss: tensor(20591432., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 215  loss: tensor(38285492., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 216  loss: tensor(84133336., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 217  loss: tensor(34551972., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 218  loss: tensor(59588816., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 219  loss: tensor(62185420., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 220  loss: tensor(53436804., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 221  loss: tensor(57474232., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 222  loss: tensor(44907672., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 223  loss: tensor(18121748., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 224  loss: tensor(70416352., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 225  loss: tensor(52938316., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 226  loss: tensor(44809992., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 227  loss: tensor(68130152., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 228  loss: tensor(39787920., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 229  loss: tensor(61315560., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 230  loss: tensor(75141272., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 231  loss: tensor(52140564., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 232  loss: tensor(43609384., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 233  loss: tensor(29529598., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 234  loss: tensor(51738224., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 235  loss: tensor(47759928., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 236  loss: tensor(38917572., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 237  loss: tensor(26417248., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 238  loss: tensor(47105092., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 239  loss: tensor(62308184., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 240  loss: tensor(65230648., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 241  loss: tensor(46722444., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 242  loss: tensor(34220860., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 243  loss: tensor(80909448., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 244  loss: tensor(39103052., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 245  loss: tensor(60920624., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 246  loss: tensor(43836048., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 247  loss: tensor(68915952., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 248  loss: tensor(58150248., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 249  loss: tensor(61801780., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 250  loss: tensor(44185804., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 251  loss: tensor(55876340., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 252  loss: tensor(27877612., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 253  loss: tensor(1.4171e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 254  loss: tensor(83618288., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 255  loss: tensor(66910180., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 256  loss: tensor(1.4978e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 257  loss: tensor(92594528., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 258  loss: tensor(68882144., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 259  loss: tensor(49656788., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 260  loss: tensor(1.1311e+08, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 261  loss: tensor(40767304., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 262  loss: tensor(37307252., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 263  loss: tensor(60046580., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 264  loss: tensor(68158096., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 265  loss: tensor(45965924., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 266  loss: tensor(40278296., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 267  loss: tensor(50308892., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 268  loss: tensor(45250976., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 269  loss: tensor(59134512., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 270  loss: tensor(40902308., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 271  loss: tensor(60341400., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 272  loss: tensor(42289208., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 273  loss: tensor(45368032., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 274  loss: tensor(17275980., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 275  loss: tensor(38574372., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 276  loss: tensor(81023472., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 277  loss: tensor(25567428., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 278  loss: tensor(57731832., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 279  loss: tensor(61237684., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 280  loss: tensor(41042708., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 281  loss: tensor(57998520., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 282  loss: tensor(28494758., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 283  loss: tensor(28623404., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 284  loss: tensor(33480444., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 285  loss: tensor(42229376., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 286  loss: tensor(46623408., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 287  loss: tensor(47687712., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 288  loss: tensor(28700704., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 289  loss: tensor(40331520., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 290  loss: tensor(59519520., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 291  loss: tensor(76957168., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 292  loss: tensor(50572520., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 293  loss: tensor(71966880., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 294  loss: tensor(53100040., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 295  loss: tensor(63411364., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 296  loss: tensor(51130572., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 297  loss: tensor(46831552., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 298  loss: tensor(45908728., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 299  loss: tensor(52785400., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 300  loss: tensor(37770532., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 301  loss: tensor(32676696., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 302  loss: tensor(34778408., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 303  loss: tensor(36761724., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 304  loss: tensor(44995496., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 305  loss: tensor(53744964., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 306  loss: tensor(45187888., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 307  loss: tensor(35190836., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 308  loss: tensor(44984352., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 309  loss: tensor(31147122., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 310  loss: tensor(60020752., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 311  loss: tensor(57089572., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 312  loss: tensor(48806536., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 313  loss: tensor(24006124., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 314  loss: tensor(33402034., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 315  loss: tensor(30887716., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 316  loss: tensor(32351206., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 317  loss: tensor(46328860., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 318  loss: tensor(48113364., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 319  loss: tensor(31911472., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 320  loss: tensor(37023260., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 321  loss: tensor(27807552., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 322  loss: tensor(48654852., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 323  loss: tensor(31344642., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 324  loss: tensor(29711354., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 325  loss: tensor(32972100., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 326  loss: tensor(36353068., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 327  loss: tensor(31923056., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 328  loss: tensor(20047762., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 329  loss: tensor(23514098., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 330  loss: tensor(75111104., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 331  loss: tensor(42199392., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 332  loss: tensor(34149700., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 333  loss: tensor(39208200., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 334  loss: tensor(40659764., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 335  loss: tensor(54047368., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 336  loss: tensor(63552076., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 337  loss: tensor(56299148., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 338  loss: tensor(27314014., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 339  loss: tensor(51077056., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 340  loss: tensor(44240160., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 341  loss: tensor(35513076., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 342  loss: tensor(32951344., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 343  loss: tensor(23006436., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 344  loss: tensor(47801736., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 345  loss: tensor(27972886., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 346  loss: tensor(20564882., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 347  loss: tensor(23165140., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 348  loss: tensor(29434728., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 349  loss: tensor(14644184., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 350  loss: tensor(55070740., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 351  loss: tensor(38603320., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 352  loss: tensor(19339886., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 353  loss: tensor(28998854., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 354  loss: tensor(39932400., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 355  loss: tensor(29600270., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 356  loss: tensor(47422220., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 357  loss: tensor(38876912., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 358  loss: tensor(38956184., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 359  loss: tensor(16519526., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 360  loss: tensor(49288344., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 361  loss: tensor(38746328., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 362  loss: tensor(40011892., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 363  loss: tensor(30764376., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 364  loss: tensor(28664032., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 365  loss: tensor(25029020., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 366  loss: tensor(7848751., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 367  loss: tensor(50520348., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 368  loss: tensor(25859252., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 369  loss: tensor(53525328., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 370  loss: tensor(75448248., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 371  loss: tensor(27342440., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 372  loss: tensor(24305926., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 373  loss: tensor(24318522., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 374  loss: tensor(29800720., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 375  loss: tensor(42238968., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 376  loss: tensor(22451692., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 377  loss: tensor(22263056., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 378  loss: tensor(33288550., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 379  loss: tensor(26735076., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 380  loss: tensor(25201422., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 381  loss: tensor(22399982., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 382  loss: tensor(22080618., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 383  loss: tensor(11024543., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 384  loss: tensor(68673536., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 385  loss: tensor(16016082., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 386  loss: tensor(25931404., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 387  loss: tensor(30838438., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 388  loss: tensor(55364684., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 389  loss: tensor(13521773., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 390  loss: tensor(10866770., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 391  loss: tensor(26029800., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 392  loss: tensor(14822434., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 393  loss: tensor(23989272., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 394  loss: tensor(22352468., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 395  loss: tensor(15807315., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 396  loss: tensor(13910212., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 397  loss: tensor(24338334., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 398  loss: tensor(50226412., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 399  loss: tensor(7152974., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 400  loss: tensor(10978446., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 401  loss: tensor(25567306., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 402  loss: tensor(51806644., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 403  loss: tensor(14887052., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 404  loss: tensor(5095093.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 405  loss: tensor(58715868., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 406  loss: tensor(13578690., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 407  loss: tensor(14822805., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 408  loss: tensor(12747294., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 409  loss: tensor(35317828., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 410  loss: tensor(17517100., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 411  loss: tensor(4432286., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 412  loss: tensor(23796664., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 413  loss: tensor(15363550., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 414  loss: tensor(13800352., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 415  loss: tensor(23481374., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 416  loss: tensor(21029036., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 417  loss: tensor(10213469., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 418  loss: tensor(30817684., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 419  loss: tensor(7546776., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 420  loss: tensor(11598667., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 421  loss: tensor(11213361., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 422  loss: tensor(8920362., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 423  loss: tensor(23677324., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 424  loss: tensor(8525898., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 425  loss: tensor(16381811., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 426  loss: tensor(16357249., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 427  loss: tensor(11652226., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 428  loss: tensor(14945794., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 429  loss: tensor(11630764., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 430  loss: tensor(17181788., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 431  loss: tensor(11124370., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 432  loss: tensor(13182983., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 433  loss: tensor(7823250., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 434  loss: tensor(10286154., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 435  loss: tensor(12690851., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 436  loss: tensor(14482142., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 437  loss: tensor(9432314., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 438  loss: tensor(12297321., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 439  loss: tensor(7410091., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 440  loss: tensor(26033176., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 441  loss: tensor(14708703., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 442  loss: tensor(18941932., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 443  loss: tensor(9977101., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 444  loss: tensor(13176031., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 445  loss: tensor(10959743., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 446  loss: tensor(17285568., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 447  loss: tensor(10319526., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 448  loss: tensor(10986250., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 449  loss: tensor(13393070., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 450  loss: tensor(7432382.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 451  loss: tensor(7475831., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 452  loss: tensor(13977382., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 453  loss: tensor(7420423., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 454  loss: tensor(13449148., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 455  loss: tensor(16372017., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 456  loss: tensor(9253412., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 457  loss: tensor(10333669., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 458  loss: tensor(3650587., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 459  loss: tensor(6997359., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 460  loss: tensor(7565379.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 461  loss: tensor(9581388., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 462  loss: tensor(7492548.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 463  loss: tensor(6991112., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 464  loss: tensor(15740704., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 465  loss: tensor(13666276., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 466  loss: tensor(6937165., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 467  loss: tensor(7211910.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 468  loss: tensor(4054738.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 469  loss: tensor(7768727., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 470  loss: tensor(3904613.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 471  loss: tensor(6150339.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 472  loss: tensor(3973256.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 473  loss: tensor(8841907., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 474  loss: tensor(4505318.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 475  loss: tensor(1553533.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 476  loss: tensor(3335153., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 477  loss: tensor(8175341.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 478  loss: tensor(7028346., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 479  loss: tensor(9111948., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 480  loss: tensor(6038882., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 481  loss: tensor(4068572.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 482  loss: tensor(8389168., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 483  loss: tensor(7329097.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 484  loss: tensor(4370114., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 485  loss: tensor(5962122., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 486  loss: tensor(11001660., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 487  loss: tensor(4414279., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 488  loss: tensor(983387.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 489  loss: tensor(5575402.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 490  loss: tensor(4198567., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 491  loss: tensor(2720394.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 492  loss: tensor(4590930., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 493  loss: tensor(3623334.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 494  loss: tensor(2362244.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 495  loss: tensor(1129334.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 496  loss: tensor(6060940.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 497  loss: tensor(4927884., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 498  loss: tensor(4415694., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 499  loss: tensor(9629846., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 500  loss: tensor(3424136.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 501  loss: tensor(3505389., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 502  loss: tensor(3107221.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 503  loss: tensor(6125095.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 504  loss: tensor(2297344.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 505  loss: tensor(2391343.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 506  loss: tensor(2687958.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 507  loss: tensor(1048490.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 508  loss: tensor(4184387., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 509  loss: tensor(12666632., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 510  loss: tensor(6542315.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 511  loss: tensor(3630800.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 512  loss: tensor(2612951., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 513  loss: tensor(3156731., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 514  loss: tensor(2377756.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 515  loss: tensor(1093893.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 516  loss: tensor(1441458.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 517  loss: tensor(1702497.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 518  loss: tensor(4156553., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 519  loss: tensor(3206290.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 520  loss: tensor(3493565., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 521  loss: tensor(1573952.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 522  loss: tensor(2995677.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 523  loss: tensor(1925732.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 524  loss: tensor(5446648.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 525  loss: tensor(983576.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 526  loss: tensor(1315959.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 527  loss: tensor(970064.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 528  loss: tensor(1564749.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 529  loss: tensor(1556631.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 530  loss: tensor(2124285.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 531  loss: tensor(3582872., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 532  loss: tensor(1977983.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 533  loss: tensor(6323471., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 534  loss: tensor(14547002., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 535  loss: tensor(1928112.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 536  loss: tensor(1212614.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 537  loss: tensor(2265146.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 538  loss: tensor(4014493.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 539  loss: tensor(2238551.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 540  loss: tensor(642634.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 541  loss: tensor(1178498.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 542  loss: tensor(350960.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 543  loss: tensor(1274086.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 544  loss: tensor(2701672.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 545  loss: tensor(1219916.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 546  loss: tensor(2237558.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 547  loss: tensor(3025856.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 548  loss: tensor(7562842., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 549  loss: tensor(2083547.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 550  loss: tensor(5560140., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 551  loss: tensor(2621909.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 552  loss: tensor(7514267., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 553  loss: tensor(1270788.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 554  loss: tensor(2340335., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 555  loss: tensor(2838370.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 556  loss: tensor(1434609.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 557  loss: tensor(717366.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 558  loss: tensor(1486643.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 559  loss: tensor(2328553.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 560  loss: tensor(1153674.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 561  loss: tensor(1580194.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 562  loss: tensor(580866.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 563  loss: tensor(4435680., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 564  loss: tensor(3370320.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 565  loss: tensor(716380.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 566  loss: tensor(2430148.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 567  loss: tensor(380626.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 568  loss: tensor(1448002.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 569  loss: tensor(3292828.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 570  loss: tensor(7426641.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 571  loss: tensor(2112713.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 572  loss: tensor(1838624., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 573  loss: tensor(2005915.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 574  loss: tensor(2925902., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 575  loss: tensor(1047969.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 576  loss: tensor(481974.8438, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 577  loss: tensor(1079484.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 578  loss: tensor(1243600.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 579  loss: tensor(1129441.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 580  loss: tensor(1800383.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 581  loss: tensor(3026963., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 582  loss: tensor(1166202.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 583  loss: tensor(1657066.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 584  loss: tensor(1064593.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 585  loss: tensor(1628717.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 586  loss: tensor(3922708.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 587  loss: tensor(996847.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 588  loss: tensor(1797487.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 589  loss: tensor(809359.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 590  loss: tensor(4537094., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 591  loss: tensor(1965419., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 592  loss: tensor(910558.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 593  loss: tensor(2312174.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 594  loss: tensor(1259075.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 595  loss: tensor(1152753.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 596  loss: tensor(479149.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 597  loss: tensor(4290096., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 598  loss: tensor(1185139.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 599  loss: tensor(699094.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 600  loss: tensor(1690030.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 601  loss: tensor(1560222., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 602  loss: tensor(2151034., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 603  loss: tensor(8526820., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 604  loss: tensor(2439534., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 605  loss: tensor(1656760.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 606  loss: tensor(2268375., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 607  loss: tensor(783129.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 608  loss: tensor(4279361., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 609  loss: tensor(4362742.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 610  loss: tensor(2855403.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 611  loss: tensor(3735478., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 612  loss: tensor(1319098.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 613  loss: tensor(1343112.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 614  loss: tensor(1556231.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 615  loss: tensor(2682790.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 616  loss: tensor(3087141., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 617  loss: tensor(670483.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 618  loss: tensor(1369819.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 619  loss: tensor(5882223., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 620  loss: tensor(1458949.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 621  loss: tensor(14115657., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 622  loss: tensor(716810.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 623  loss: tensor(1533362., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 624  loss: tensor(1639004.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 625  loss: tensor(1534242.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 626  loss: tensor(1491247.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 627  loss: tensor(1353272.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 628  loss: tensor(1629892.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 629  loss: tensor(532538.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 630  loss: tensor(1866492.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 631  loss: tensor(966707.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 632  loss: tensor(1975373., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 633  loss: tensor(7594294.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 634  loss: tensor(806591.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 635  loss: tensor(1612486.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 636  loss: tensor(4411704., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 637  loss: tensor(972496.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 638  loss: tensor(3052202.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 639  loss: tensor(5483097.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 640  loss: tensor(2124044., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 641  loss: tensor(2141689.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 642  loss: tensor(983421.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 643  loss: tensor(6271255., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 644  loss: tensor(7562474., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 645  loss: tensor(6711994.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 646  loss: tensor(4818161., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 647  loss: tensor(2360637.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 648  loss: tensor(2587079.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 649  loss: tensor(838993.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 650  loss: tensor(819061.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 651  loss: tensor(2244807.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 652  loss: tensor(530090.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 653  loss: tensor(3783628., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 654  loss: tensor(1614105.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 655  loss: tensor(1271107.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 656  loss: tensor(865653., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 657  loss: tensor(986518.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 658  loss: tensor(2635407.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 659  loss: tensor(5174843., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 660  loss: tensor(587172.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 661  loss: tensor(894587.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 662  loss: tensor(2169802., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 663  loss: tensor(2987598., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 664  loss: tensor(472608.5312, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 665  loss: tensor(407790.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 666  loss: tensor(734518., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 667  loss: tensor(2439492.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 668  loss: tensor(1373860.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 669  loss: tensor(1246702.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 670  loss: tensor(2927403.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 671  loss: tensor(3328334.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 672  loss: tensor(12873380., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 673  loss: tensor(19821558., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 674  loss: tensor(3065456.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 675  loss: tensor(2060624.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 676  loss: tensor(5544536., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 677  loss: tensor(7586240., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 678  loss: tensor(1671579.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 679  loss: tensor(1898609.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 680  loss: tensor(3024698.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 681  loss: tensor(1679297.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 682  loss: tensor(570234.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 683  loss: tensor(587635.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 684  loss: tensor(4013717.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 685  loss: tensor(9446563., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 686  loss: tensor(7426985.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 687  loss: tensor(2639330.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 688  loss: tensor(3134080.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 689  loss: tensor(2616865.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 690  loss: tensor(1341309.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 691  loss: tensor(2483060.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 692  loss: tensor(602610.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 693  loss: tensor(2059081.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 694  loss: tensor(1116003.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 695  loss: tensor(2771776., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 696  loss: tensor(1236763.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 697  loss: tensor(2319869.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 698  loss: tensor(4053539.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 699  loss: tensor(2144552.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 700  loss: tensor(3776569.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 701  loss: tensor(6819790., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 702  loss: tensor(1066076.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 703  loss: tensor(847167.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 704  loss: tensor(442735.5312, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 705  loss: tensor(1331127., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 706  loss: tensor(1532899.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 707  loss: tensor(779794.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 708  loss: tensor(1149585., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 709  loss: tensor(2239362., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 710  loss: tensor(669357.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 711  loss: tensor(3657731., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 712  loss: tensor(2471397., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 713  loss: tensor(3422284., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 714  loss: tensor(2673068.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 715  loss: tensor(1276198., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 716  loss: tensor(895682.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 717  loss: tensor(418786.7188, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 718  loss: tensor(4055544.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 719  loss: tensor(3318419.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 720  loss: tensor(1259842.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 721  loss: tensor(3177612.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 722  loss: tensor(2710609.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 723  loss: tensor(846317.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 724  loss: tensor(1852644.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 725  loss: tensor(1533612.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 726  loss: tensor(945998.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 727  loss: tensor(936504.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 728  loss: tensor(1558183.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 729  loss: tensor(1728083.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 730  loss: tensor(2697133.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 731  loss: tensor(9163556., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 732  loss: tensor(6075645.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 733  loss: tensor(3106291., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 734  loss: tensor(5995769., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 735  loss: tensor(2564969.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 736  loss: tensor(2977644.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 737  loss: tensor(2643795., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 738  loss: tensor(3573563., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 739  loss: tensor(1417251.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 740  loss: tensor(435365.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 741  loss: tensor(3310750.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 742  loss: tensor(926443.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 743  loss: tensor(2117088.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 744  loss: tensor(3514471., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 745  loss: tensor(1832465.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 746  loss: tensor(2242102.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 747  loss: tensor(912642., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 748  loss: tensor(1596802.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 749  loss: tensor(913998.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 750  loss: tensor(913240.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 751  loss: tensor(2380194.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 752  loss: tensor(922167.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 753  loss: tensor(1387599.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 754  loss: tensor(6066015.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 755  loss: tensor(1766719.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 756  loss: tensor(6587527., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 757  loss: tensor(1046727.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 758  loss: tensor(2496981.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 759  loss: tensor(2872223.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 760  loss: tensor(799882.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 761  loss: tensor(1908228., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 762  loss: tensor(821526.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 763  loss: tensor(7300799.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 764  loss: tensor(4813306.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 765  loss: tensor(4273999., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 766  loss: tensor(5339519., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 767  loss: tensor(370549.7188, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 768  loss: tensor(2592851.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 769  loss: tensor(1703313.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 770  loss: tensor(2803439.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 771  loss: tensor(3542889.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 772  loss: tensor(3459317.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 773  loss: tensor(5604104.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 774  loss: tensor(1503236.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 775  loss: tensor(941909.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 776  loss: tensor(1715287.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 777  loss: tensor(15879853., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 778  loss: tensor(5651904.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 779  loss: tensor(1924441.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 780  loss: tensor(356834.9062, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 781  loss: tensor(1095560.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 782  loss: tensor(834256.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 783  loss: tensor(1011238.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 784  loss: tensor(1632357.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 785  loss: tensor(1316958.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 786  loss: tensor(6614382., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 787  loss: tensor(6462405.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 788  loss: tensor(2995626.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 789  loss: tensor(4931910.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 790  loss: tensor(2499695., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 791  loss: tensor(544781.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 792  loss: tensor(3385612.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 793  loss: tensor(10481267., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 794  loss: tensor(1912022., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 795  loss: tensor(934358.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 796  loss: tensor(1009848.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 797  loss: tensor(5163801., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 798  loss: tensor(2012644.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 799  loss: tensor(4100297.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 800  loss: tensor(1285935.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 801  loss: tensor(572414.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 802  loss: tensor(2758583., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 803  loss: tensor(1023095.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 804  loss: tensor(3758468., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 805  loss: tensor(2691486., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 806  loss: tensor(1755109.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 807  loss: tensor(680800.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 808  loss: tensor(4166858., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 809  loss: tensor(2190969.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 810  loss: tensor(1913273.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 811  loss: tensor(1074816.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 812  loss: tensor(1160078.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 813  loss: tensor(2295682., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 814  loss: tensor(1177049.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 815  loss: tensor(3564447.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 816  loss: tensor(1013007.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 817  loss: tensor(1394498.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 818  loss: tensor(2730362., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 819  loss: tensor(1010862.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 820  loss: tensor(3317311.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 821  loss: tensor(1340660.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 822  loss: tensor(1419424.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 823  loss: tensor(1958706., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 824  loss: tensor(837171., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 825  loss: tensor(167136.7969, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 826  loss: tensor(2508334.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 827  loss: tensor(4921719.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 828  loss: tensor(927395.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 829  loss: tensor(3768637.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 830  loss: tensor(3663521.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 831  loss: tensor(680125.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 832  loss: tensor(1880750., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 833  loss: tensor(2245044.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 834  loss: tensor(678355.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 835  loss: tensor(1790653.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 836  loss: tensor(1906693.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 837  loss: tensor(866157.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 838  loss: tensor(3520615., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 839  loss: tensor(828828.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 840  loss: tensor(1766192.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 841  loss: tensor(720878.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 842  loss: tensor(1480647., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 843  loss: tensor(2443832.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 844  loss: tensor(2990207.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 845  loss: tensor(4142579.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 846  loss: tensor(998992.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 847  loss: tensor(1721009.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 848  loss: tensor(450415.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 849  loss: tensor(3703987.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 850  loss: tensor(5950918., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 851  loss: tensor(1621281.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 852  loss: tensor(1108222.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 853  loss: tensor(1673723.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 854  loss: tensor(5430618.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 855  loss: tensor(1926149.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 856  loss: tensor(296090.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 857  loss: tensor(5451682., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 858  loss: tensor(2187033., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 859  loss: tensor(2501996.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 860  loss: tensor(637468.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 861  loss: tensor(4258281., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 862  loss: tensor(3042872.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 863  loss: tensor(2696159.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 864  loss: tensor(7918251.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 865  loss: tensor(3074036.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 866  loss: tensor(1078047., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 867  loss: tensor(1246036.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 868  loss: tensor(12395039., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 869  loss: tensor(3214039.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 870  loss: tensor(1473803.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 871  loss: tensor(2694179.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 872  loss: tensor(4296800., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 873  loss: tensor(3241166.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 874  loss: tensor(7592894.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 875  loss: tensor(3791112., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 876  loss: tensor(23800134., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 877  loss: tensor(2671211., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 878  loss: tensor(1709861.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 879  loss: tensor(1632230., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 880  loss: tensor(2199656., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 881  loss: tensor(8276783., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 882  loss: tensor(1037443.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 883  loss: tensor(1702493., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 884  loss: tensor(1421777., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 885  loss: tensor(5274553.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 886  loss: tensor(3140779.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 887  loss: tensor(1920704.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 888  loss: tensor(1331337., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 889  loss: tensor(1090642.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 890  loss: tensor(1814662.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 891  loss: tensor(1136805.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 892  loss: tensor(1706871.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 893  loss: tensor(1423010.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 894  loss: tensor(1318903.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 895  loss: tensor(1662624., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 896  loss: tensor(1085225.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 897  loss: tensor(1625843.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 898  loss: tensor(2267359., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 899  loss: tensor(581979.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 900  loss: tensor(1352353.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 901  loss: tensor(1993930.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 902  loss: tensor(4581462., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 903  loss: tensor(1649444.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 904  loss: tensor(3094140.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 905  loss: tensor(5338719., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 906  loss: tensor(964557.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 907  loss: tensor(706184.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 908  loss: tensor(4256817.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 909  loss: tensor(1406951.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 910  loss: tensor(2956845.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 911  loss: tensor(8131437.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 912  loss: tensor(1069144.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 913  loss: tensor(1083712.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 914  loss: tensor(3392724.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 915  loss: tensor(22291698., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 916  loss: tensor(527587.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 917  loss: tensor(1257054.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 918  loss: tensor(690877.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 919  loss: tensor(5346318., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 920  loss: tensor(2281665.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 921  loss: tensor(1855139.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 922  loss: tensor(966037.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 923  loss: tensor(401764.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 924  loss: tensor(987065.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 925  loss: tensor(644957.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 926  loss: tensor(591259.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 927  loss: tensor(4308867., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 928  loss: tensor(2236318.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 929  loss: tensor(525768.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 930  loss: tensor(821234.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 931  loss: tensor(2303913., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 932  loss: tensor(2179080.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 933  loss: tensor(3150118., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 934  loss: tensor(2504134.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 935  loss: tensor(3291881., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 936  loss: tensor(1477773., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 937  loss: tensor(1331512.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 938  loss: tensor(1901960., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 939  loss: tensor(8646395., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 940  loss: tensor(2860243.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 941  loss: tensor(1682121.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 942  loss: tensor(356080.4062, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 943  loss: tensor(1032767.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 944  loss: tensor(2050650.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 945  loss: tensor(1255345.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 946  loss: tensor(7621460.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 947  loss: tensor(2307741.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 948  loss: tensor(696007.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 949  loss: tensor(589578.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 950  loss: tensor(2676305.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 951  loss: tensor(1321417.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 952  loss: tensor(2533070.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 953  loss: tensor(2969418.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 954  loss: tensor(502522., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 955  loss: tensor(1394972.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 956  loss: tensor(2082531.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 957  loss: tensor(605476.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 958  loss: tensor(1563942.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 959  loss: tensor(559865.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 960  loss: tensor(2663098.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 961  loss: tensor(3410845., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 962  loss: tensor(1782209.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 963  loss: tensor(3894262.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 964  loss: tensor(1105840., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 965  loss: tensor(2148128.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 966  loss: tensor(1137045.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 967  loss: tensor(4406411., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 968  loss: tensor(2544385.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 969  loss: tensor(2416397., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 970  loss: tensor(3588285.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 971  loss: tensor(9144155., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 972  loss: tensor(10058266., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 973  loss: tensor(625855.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 974  loss: tensor(1771343.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 975  loss: tensor(6307649.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 976  loss: tensor(1172154.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 977  loss: tensor(7309059.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 978  loss: tensor(5942446., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 979  loss: tensor(818480.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 980  loss: tensor(938911.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 981  loss: tensor(6073278.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 982  loss: tensor(2372465., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 983  loss: tensor(1248381.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 984  loss: tensor(2805491., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 985  loss: tensor(3073779., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 986  loss: tensor(632610.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 987  loss: tensor(882454.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 988  loss: tensor(1056914.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 989  loss: tensor(5637091., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 990  loss: tensor(1715686.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 991  loss: tensor(2117855.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 992  loss: tensor(1555450.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 993  loss: tensor(1671421.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 994  loss: tensor(2529717.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 995  loss: tensor(449780.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 996  loss: tensor(529744.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 997  loss: tensor(1905289.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 998  loss: tensor(1232981.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 999  loss: tensor(846107., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1000  loss: tensor(2680645.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1001  loss: tensor(921056.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1002  loss: tensor(629055.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1003  loss: tensor(330636.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1004  loss: tensor(2452307.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1005  loss: tensor(1118965.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1006  loss: tensor(2269745.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1007  loss: tensor(1381004., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1008  loss: tensor(3462663., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1009  loss: tensor(552034.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1010  loss: tensor(1959353.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1011  loss: tensor(3079674.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1012  loss: tensor(1974959.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1013  loss: tensor(1971821.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1014  loss: tensor(1169708.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1015  loss: tensor(5819100.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1016  loss: tensor(1088325.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1017  loss: tensor(1254228.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1018  loss: tensor(1372459.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1019  loss: tensor(2053319., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1020  loss: tensor(4552915., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1021  loss: tensor(1497547., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1022  loss: tensor(3241541.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1023  loss: tensor(7430859.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1024  loss: tensor(2913715.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1025  loss: tensor(1339676.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1026  loss: tensor(6094708., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1027  loss: tensor(2363603.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1028  loss: tensor(781951.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1029  loss: tensor(3799458., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1030  loss: tensor(1532786.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1031  loss: tensor(4439739.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1032  loss: tensor(718153., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1033  loss: tensor(5976166., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1034  loss: tensor(2835685.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1035  loss: tensor(2173705.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1036  loss: tensor(1831535.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1037  loss: tensor(546324.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1038  loss: tensor(2101855., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1039  loss: tensor(647761.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1040  loss: tensor(889595., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1041  loss: tensor(3013852.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1042  loss: tensor(2208962.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1043  loss: tensor(638353., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1044  loss: tensor(728136.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1045  loss: tensor(962756.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1046  loss: tensor(1353603.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1047  loss: tensor(1016070.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1048  loss: tensor(1321195.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1049  loss: tensor(523626.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1050  loss: tensor(1620265.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1051  loss: tensor(3046698.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1052  loss: tensor(2409172.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1053  loss: tensor(445432.7188, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1054  loss: tensor(2239375.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1055  loss: tensor(4516657.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1056  loss: tensor(2267312.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1057  loss: tensor(1432675.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1058  loss: tensor(984833.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1059  loss: tensor(5140020., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1060  loss: tensor(278330.2812, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1061  loss: tensor(1425179.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1062  loss: tensor(6164354., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1063  loss: tensor(1671142.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1064  loss: tensor(2769109., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1065  loss: tensor(2997002.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1066  loss: tensor(5866872.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1067  loss: tensor(1003297.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1068  loss: tensor(622787.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1069  loss: tensor(1375736.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1070  loss: tensor(3574737.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1071  loss: tensor(1602092.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1072  loss: tensor(2729397.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1073  loss: tensor(2058045.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1074  loss: tensor(295790.0938, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1075  loss: tensor(3295101., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1076  loss: tensor(1032031.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1077  loss: tensor(170679.2344, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1078  loss: tensor(743260.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1079  loss: tensor(2708573.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1080  loss: tensor(1629131.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1081  loss: tensor(3365044.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1082  loss: tensor(1508755.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1083  loss: tensor(3701641.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1084  loss: tensor(4505765.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1085  loss: tensor(428235.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1086  loss: tensor(2046460.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1087  loss: tensor(537623.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1088  loss: tensor(1665153.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1089  loss: tensor(992295.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1090  loss: tensor(4035908., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1091  loss: tensor(1626829.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1092  loss: tensor(5053782., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1093  loss: tensor(2857385.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1094  loss: tensor(911048.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1095  loss: tensor(11997654., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1096  loss: tensor(899264.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1097  loss: tensor(1726782.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1098  loss: tensor(1471708.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1099  loss: tensor(8503865., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1100  loss: tensor(3987013.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1101  loss: tensor(7025495., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1102  loss: tensor(2789053.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1103  loss: tensor(1755366.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1104  loss: tensor(3253353.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1105  loss: tensor(1693075.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1106  loss: tensor(1250199., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1107  loss: tensor(1837480.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1108  loss: tensor(1702462.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1109  loss: tensor(4696363.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1110  loss: tensor(870929.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1111  loss: tensor(1738082.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1112  loss: tensor(1134949.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1113  loss: tensor(1590046.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1114  loss: tensor(1423355.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1115  loss: tensor(1117631.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1116  loss: tensor(1748970.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1117  loss: tensor(1812752.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1118  loss: tensor(3459646.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1119  loss: tensor(2343003.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1120  loss: tensor(1982034.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1121  loss: tensor(1900296.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1122  loss: tensor(1588047.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1123  loss: tensor(2506749., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1124  loss: tensor(1177748.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1125  loss: tensor(5219902., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1126  loss: tensor(3548280., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1127  loss: tensor(1356829.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1128  loss: tensor(469770.4688, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1129  loss: tensor(971539.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1130  loss: tensor(889312.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1131  loss: tensor(1149498.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1132  loss: tensor(2433945., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1133  loss: tensor(1144751.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1134  loss: tensor(2661436., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1135  loss: tensor(4783088., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1136  loss: tensor(563034.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1137  loss: tensor(610147.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1138  loss: tensor(909828.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1139  loss: tensor(629302.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1140  loss: tensor(736608.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1141  loss: tensor(1119622.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1142  loss: tensor(660081.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1143  loss: tensor(685816.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1144  loss: tensor(1197686.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1145  loss: tensor(1383634.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1146  loss: tensor(1395455., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1147  loss: tensor(976742.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1148  loss: tensor(716819.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1149  loss: tensor(942976.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1150  loss: tensor(1189114.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1151  loss: tensor(934706.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1152  loss: tensor(15399280., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1153  loss: tensor(1210495.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1154  loss: tensor(1073011.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1155  loss: tensor(1614396.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1156  loss: tensor(1662726.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1157  loss: tensor(1212620., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1158  loss: tensor(3754760.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1159  loss: tensor(4373534.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1160  loss: tensor(3773666.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1161  loss: tensor(859190.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1162  loss: tensor(3473173., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1163  loss: tensor(2872519.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1164  loss: tensor(1094609.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1165  loss: tensor(6093135., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1166  loss: tensor(1067051.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1167  loss: tensor(572065.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1168  loss: tensor(291226.8438, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1169  loss: tensor(6591225., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1170  loss: tensor(2393860.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1171  loss: tensor(2442292.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1172  loss: tensor(3724279., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1173  loss: tensor(1831074., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1174  loss: tensor(1236509.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1175  loss: tensor(24943702., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1176  loss: tensor(2410525., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1177  loss: tensor(2321637., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1178  loss: tensor(913612.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1179  loss: tensor(1724950.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1180  loss: tensor(1829620.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1181  loss: tensor(1589995.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1182  loss: tensor(1467017., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1183  loss: tensor(1531692.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1184  loss: tensor(1996673.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1185  loss: tensor(1484917., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1186  loss: tensor(2852911., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1187  loss: tensor(1516871., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1188  loss: tensor(1426869., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1189  loss: tensor(1239482., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1190  loss: tensor(2101622.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1191  loss: tensor(752303.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1192  loss: tensor(1180815., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1193  loss: tensor(3243619.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1194  loss: tensor(690949.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1195  loss: tensor(4368265., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1196  loss: tensor(878371.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1197  loss: tensor(529932.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1198  loss: tensor(6176633., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1199  loss: tensor(6766391.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1200  loss: tensor(2551331.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1201  loss: tensor(2454268.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1202  loss: tensor(3073778.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1203  loss: tensor(6827246., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1204  loss: tensor(3200401., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1205  loss: tensor(2117484., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1206  loss: tensor(1351802., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1207  loss: tensor(1716343., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1208  loss: tensor(609216.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1209  loss: tensor(11338738., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1210  loss: tensor(1640792.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1211  loss: tensor(1236580.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1212  loss: tensor(2290488.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1213  loss: tensor(1717433.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1214  loss: tensor(1069444.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1215  loss: tensor(266185.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1216  loss: tensor(1674676.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1217  loss: tensor(888268.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1218  loss: tensor(1582480.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1219  loss: tensor(932798.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1220  loss: tensor(864431.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1221  loss: tensor(1338509., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1222  loss: tensor(862148.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1223  loss: tensor(926389.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1224  loss: tensor(9026832., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1225  loss: tensor(963052.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1226  loss: tensor(2896827.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1227  loss: tensor(1032082.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1228  loss: tensor(1109173.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1229  loss: tensor(954000.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1230  loss: tensor(1114649.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1231  loss: tensor(1451455., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1232  loss: tensor(2140102.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1233  loss: tensor(980395.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1234  loss: tensor(2397948.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1235  loss: tensor(1072554.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1236  loss: tensor(449385.6562, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1237  loss: tensor(1468578.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1238  loss: tensor(2689330., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1239  loss: tensor(527071., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1240  loss: tensor(2659445.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1241  loss: tensor(2221767., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1242  loss: tensor(600074.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1243  loss: tensor(1953198.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1244  loss: tensor(3938089.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1245  loss: tensor(6781337.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1246  loss: tensor(1959434., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1247  loss: tensor(618485., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1248  loss: tensor(307943.7188, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1249  loss: tensor(1651044.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1250  loss: tensor(13241482., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1251  loss: tensor(1956655.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1252  loss: tensor(2508154., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1253  loss: tensor(743453.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1254  loss: tensor(1140887.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1255  loss: tensor(1363107.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1256  loss: tensor(2608455.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1257  loss: tensor(1347666.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1258  loss: tensor(769071.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1259  loss: tensor(4025296.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1260  loss: tensor(3364463.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1261  loss: tensor(2802492.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1262  loss: tensor(2641354., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1263  loss: tensor(1505857., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1264  loss: tensor(1467659.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1265  loss: tensor(3079017.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1266  loss: tensor(3954903., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1267  loss: tensor(578466.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1268  loss: tensor(3107616., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1269  loss: tensor(8302042., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1270  loss: tensor(5047321., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1271  loss: tensor(10039595., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1272  loss: tensor(3995388.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1273  loss: tensor(2481740.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1274  loss: tensor(2299790.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1275  loss: tensor(1059999.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1276  loss: tensor(1290695.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1277  loss: tensor(1707132.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1278  loss: tensor(1605778.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1279  loss: tensor(5570140., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1280  loss: tensor(6533689., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1281  loss: tensor(1031843.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1282  loss: tensor(2111305., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1283  loss: tensor(2640195., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1284  loss: tensor(1693371.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1285  loss: tensor(1985540.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1286  loss: tensor(572754.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1287  loss: tensor(944148.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1288  loss: tensor(1613798.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1289  loss: tensor(3109036.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1290  loss: tensor(1159933.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1291  loss: tensor(8316430.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1292  loss: tensor(703642.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1293  loss: tensor(466451.9062, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1294  loss: tensor(3002490.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1295  loss: tensor(5076559., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1296  loss: tensor(1021118.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1297  loss: tensor(2469728.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1298  loss: tensor(1898770.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1299  loss: tensor(736258.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1300  loss: tensor(1149733.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1301  loss: tensor(1378134., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1302  loss: tensor(983614., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1303  loss: tensor(5954037., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1304  loss: tensor(5055034.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1305  loss: tensor(2163193.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1306  loss: tensor(954797.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1307  loss: tensor(5310263., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1308  loss: tensor(797979.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1309  loss: tensor(3249047.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1310  loss: tensor(5866040., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1311  loss: tensor(2372977.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1312  loss: tensor(2796164.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1313  loss: tensor(1108038.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1314  loss: tensor(5460076.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1315  loss: tensor(4355072., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1316  loss: tensor(1459071.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1317  loss: tensor(6341606.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1318  loss: tensor(541836.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1319  loss: tensor(387802.9688, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1320  loss: tensor(11137874., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1321  loss: tensor(8259422., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1322  loss: tensor(995993.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1323  loss: tensor(6494806.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1324  loss: tensor(5095247., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1325  loss: tensor(203224.2344, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1326  loss: tensor(1715965.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1327  loss: tensor(1203014.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1328  loss: tensor(897898.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1329  loss: tensor(1027330.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1330  loss: tensor(1625271.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1331  loss: tensor(1282306.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1332  loss: tensor(757897.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1333  loss: tensor(1110348.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1334  loss: tensor(1991247.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1335  loss: tensor(1412955., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1336  loss: tensor(436026.9688, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1337  loss: tensor(10005984., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1338  loss: tensor(2707231.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1339  loss: tensor(6054977.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1340  loss: tensor(5033263.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1341  loss: tensor(1376498.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1342  loss: tensor(1090881.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1343  loss: tensor(1643100.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1344  loss: tensor(6760947., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1345  loss: tensor(2688418.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1346  loss: tensor(1857768.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1347  loss: tensor(835793.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1348  loss: tensor(3555359.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1349  loss: tensor(2469869., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1350  loss: tensor(663394.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1351  loss: tensor(5387259.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1352  loss: tensor(1202066.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1353  loss: tensor(1632998.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1354  loss: tensor(294894.7188, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1355  loss: tensor(1256538.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1356  loss: tensor(416993.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1357  loss: tensor(4458171., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1358  loss: tensor(1348046.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1359  loss: tensor(1313360.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1360  loss: tensor(3200641.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1361  loss: tensor(3060720., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1362  loss: tensor(1804449., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1363  loss: tensor(669921.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1364  loss: tensor(710724.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1365  loss: tensor(5509455.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1366  loss: tensor(1711893.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1367  loss: tensor(1320038.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1368  loss: tensor(1987662.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1369  loss: tensor(1342808.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1370  loss: tensor(1154966.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1371  loss: tensor(1404629.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1372  loss: tensor(4968213., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1373  loss: tensor(2834963.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1374  loss: tensor(2037436.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1375  loss: tensor(4499802.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1376  loss: tensor(2885666., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1377  loss: tensor(955134.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1378  loss: tensor(2511032.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1379  loss: tensor(2359161.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1380  loss: tensor(1413176.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1381  loss: tensor(2692095.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1382  loss: tensor(2391418.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1383  loss: tensor(2840646.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1384  loss: tensor(2622527., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1385  loss: tensor(850317.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1386  loss: tensor(1207383., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1387  loss: tensor(601529.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1388  loss: tensor(1800716.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1389  loss: tensor(5210913.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1390  loss: tensor(2452149., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1391  loss: tensor(814124.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1392  loss: tensor(2596133.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1393  loss: tensor(3296082.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1394  loss: tensor(1038729.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1395  loss: tensor(3666271.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1396  loss: tensor(9932054., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1397  loss: tensor(14647981., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1398  loss: tensor(19003826., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1399  loss: tensor(835973.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1400  loss: tensor(4445555.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1401  loss: tensor(10368947., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1402  loss: tensor(14204989., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1403  loss: tensor(6688069.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1404  loss: tensor(2192000.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1405  loss: tensor(3608444.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1406  loss: tensor(8543924., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1407  loss: tensor(1928495.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1408  loss: tensor(2326704., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1409  loss: tensor(2596172.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1410  loss: tensor(7288615.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1411  loss: tensor(1616199.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1412  loss: tensor(4193237.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1413  loss: tensor(4661411.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1414  loss: tensor(2168341.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1415  loss: tensor(2851135.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1416  loss: tensor(2501140., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1417  loss: tensor(597533.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1418  loss: tensor(1462745.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1419  loss: tensor(1147691.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1420  loss: tensor(998799.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1421  loss: tensor(2167972.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1422  loss: tensor(2606747.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1423  loss: tensor(2978437., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1424  loss: tensor(9135306., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1425  loss: tensor(6852011., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1426  loss: tensor(1772569.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1427  loss: tensor(2319784., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1428  loss: tensor(934770., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1429  loss: tensor(1829535.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1430  loss: tensor(878940.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1431  loss: tensor(1153205.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1432  loss: tensor(4502012.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1433  loss: tensor(2431903.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1434  loss: tensor(207367.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1435  loss: tensor(2451129.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1436  loss: tensor(3791977.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1437  loss: tensor(4163532.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1438  loss: tensor(1120887.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1439  loss: tensor(2591838.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1440  loss: tensor(1355636., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1441  loss: tensor(4160929., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1442  loss: tensor(1464182.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1443  loss: tensor(4166980., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1444  loss: tensor(3739859., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1445  loss: tensor(1523375.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1446  loss: tensor(1496446.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1447  loss: tensor(1115599.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1448  loss: tensor(1536186.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1449  loss: tensor(1654663.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1450  loss: tensor(2054616.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1451  loss: tensor(7781821., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1452  loss: tensor(1061492.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1453  loss: tensor(1190553.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1454  loss: tensor(2134797.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1455  loss: tensor(1102998.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1456  loss: tensor(1257731.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1457  loss: tensor(7527818.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1458  loss: tensor(2322225.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1459  loss: tensor(11292575., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1460  loss: tensor(2248357.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1461  loss: tensor(554594.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1462  loss: tensor(1643291., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1463  loss: tensor(11312389., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1464  loss: tensor(1236974.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1465  loss: tensor(3308093., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1466  loss: tensor(670757., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1467  loss: tensor(5146268.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1468  loss: tensor(3134892.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1469  loss: tensor(914262.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1470  loss: tensor(1162710.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1471  loss: tensor(4342870.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1472  loss: tensor(6705593., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1473  loss: tensor(2143863.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1474  loss: tensor(1003610.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1475  loss: tensor(4591064.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1476  loss: tensor(1573288.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1477  loss: tensor(3790033.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1478  loss: tensor(5023714., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1479  loss: tensor(1250087.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1480  loss: tensor(10466474., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1481  loss: tensor(2048535.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1482  loss: tensor(2843628.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1483  loss: tensor(6173657.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1484  loss: tensor(965713., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1485  loss: tensor(1960072.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1486  loss: tensor(4439042., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1487  loss: tensor(4825929.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1488  loss: tensor(1689863.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1489  loss: tensor(1592894., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1490  loss: tensor(398044.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1491  loss: tensor(4774906.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1492  loss: tensor(2431362.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1493  loss: tensor(6517724., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1494  loss: tensor(1634228., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1495  loss: tensor(10213335., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1496  loss: tensor(12992673., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1497  loss: tensor(4799347., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1498  loss: tensor(4981638., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1499  loss: tensor(3999966., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1500  loss: tensor(848152.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1501  loss: tensor(897413.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1502  loss: tensor(3157182.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1503  loss: tensor(861692.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1504  loss: tensor(5794625., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1505  loss: tensor(1420408.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1506  loss: tensor(992328.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1507  loss: tensor(5275023., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1508  loss: tensor(934601.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1509  loss: tensor(1052360.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1510  loss: tensor(1173199., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1511  loss: tensor(6031917.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1512  loss: tensor(3378618.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1513  loss: tensor(2301760., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1514  loss: tensor(2738012., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1515  loss: tensor(1584985.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1516  loss: tensor(1833860.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1517  loss: tensor(633832.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1518  loss: tensor(561363.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1519  loss: tensor(7166675., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1520  loss: tensor(3214940.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1521  loss: tensor(312420.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1522  loss: tensor(5581673.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1523  loss: tensor(14036848., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1524  loss: tensor(10612694., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1525  loss: tensor(5654169.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1526  loss: tensor(3637402.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1527  loss: tensor(897597.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1528  loss: tensor(1779297., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1529  loss: tensor(742467.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1530  loss: tensor(2950861.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1531  loss: tensor(1082328.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1532  loss: tensor(1102398.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1533  loss: tensor(1120316.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1534  loss: tensor(913288.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1535  loss: tensor(1045607.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1536  loss: tensor(1278632.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1537  loss: tensor(1636234.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1538  loss: tensor(1667876.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1539  loss: tensor(4539989.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1540  loss: tensor(892553., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1541  loss: tensor(1132790.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1542  loss: tensor(919948., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1543  loss: tensor(1446711.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1544  loss: tensor(700456.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1545  loss: tensor(1233548.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1546  loss: tensor(13599016., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1547  loss: tensor(4778458., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1548  loss: tensor(10598422., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1549  loss: tensor(7231526.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1550  loss: tensor(1898461., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1551  loss: tensor(2025700.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1552  loss: tensor(506626.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1553  loss: tensor(7345138.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1554  loss: tensor(1425365.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1555  loss: tensor(3426597.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1556  loss: tensor(1830440.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1557  loss: tensor(1741206.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1558  loss: tensor(1814160.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1559  loss: tensor(2975910.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1560  loss: tensor(1436186., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1561  loss: tensor(1357847.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1562  loss: tensor(525553.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1563  loss: tensor(873363.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1564  loss: tensor(2528240., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1565  loss: tensor(1669140.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1566  loss: tensor(703508., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1567  loss: tensor(4761589.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1568  loss: tensor(921759.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1569  loss: tensor(1062775., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1570  loss: tensor(1177421.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1571  loss: tensor(3784914.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1572  loss: tensor(338011., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1573  loss: tensor(3005767.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1574  loss: tensor(2648880.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1575  loss: tensor(3199079.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1576  loss: tensor(938390.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1577  loss: tensor(1912561.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1578  loss: tensor(584742.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1579  loss: tensor(1521149.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1580  loss: tensor(2152528.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1581  loss: tensor(437325.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1582  loss: tensor(988494.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1583  loss: tensor(732850.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1584  loss: tensor(1945145.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1585  loss: tensor(531285.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1586  loss: tensor(1249367.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1587  loss: tensor(6444180.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1588  loss: tensor(2522344.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1589  loss: tensor(1425155.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1590  loss: tensor(554549.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1591  loss: tensor(1082501.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1592  loss: tensor(2992405., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1593  loss: tensor(1155818.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1594  loss: tensor(1798838.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1595  loss: tensor(7581330., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1596  loss: tensor(1519607.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1597  loss: tensor(1433217.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1598  loss: tensor(1685587.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1599  loss: tensor(675087.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1600  loss: tensor(3377305., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1601  loss: tensor(1810243.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1602  loss: tensor(7214159.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1603  loss: tensor(3147327.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1604  loss: tensor(1614141.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1605  loss: tensor(1269058., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1606  loss: tensor(2503204.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1607  loss: tensor(3856799.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1608  loss: tensor(3902553., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1609  loss: tensor(7418732.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1610  loss: tensor(4871055.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1611  loss: tensor(1616466.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1612  loss: tensor(3305526., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1613  loss: tensor(1459216.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1614  loss: tensor(7266188.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1615  loss: tensor(3501395., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1616  loss: tensor(1190455.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1617  loss: tensor(2763728.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1618  loss: tensor(1048848.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1619  loss: tensor(7326881., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1620  loss: tensor(930684., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1621  loss: tensor(1247624.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1622  loss: tensor(2972224.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1623  loss: tensor(1083851.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1624  loss: tensor(4523443.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1625  loss: tensor(2756866.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1626  loss: tensor(1574886.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1627  loss: tensor(767924.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1628  loss: tensor(2941996.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1629  loss: tensor(4894227., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1630  loss: tensor(2898397., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1631  loss: tensor(1544158.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1632  loss: tensor(1313578.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1633  loss: tensor(4084748.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1634  loss: tensor(2838057.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1635  loss: tensor(5920617.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1636  loss: tensor(14817367., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1637  loss: tensor(6672532.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1638  loss: tensor(14403213., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1639  loss: tensor(8645063., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1640  loss: tensor(956827.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1641  loss: tensor(1849834.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1642  loss: tensor(8510955., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1643  loss: tensor(8325550.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1644  loss: tensor(709255.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1645  loss: tensor(972725.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1646  loss: tensor(3063244.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1647  loss: tensor(2985650.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1648  loss: tensor(1403813.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1649  loss: tensor(1494510., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1650  loss: tensor(4266745.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1651  loss: tensor(6720799.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1652  loss: tensor(6274163., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1653  loss: tensor(2846450.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1654  loss: tensor(1159960.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1655  loss: tensor(3116195.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1656  loss: tensor(4843183., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1657  loss: tensor(3019195.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1658  loss: tensor(4178962.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1659  loss: tensor(2091715.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1660  loss: tensor(1345240.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1661  loss: tensor(3015894.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1662  loss: tensor(3468046.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1663  loss: tensor(602039.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1664  loss: tensor(2460001.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1665  loss: tensor(1666167.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1666  loss: tensor(2246077.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1667  loss: tensor(2039809.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1668  loss: tensor(542637.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1669  loss: tensor(3258149.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1670  loss: tensor(1841909.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1671  loss: tensor(4313655.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1672  loss: tensor(1066642.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1673  loss: tensor(5660719., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1674  loss: tensor(7246432.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1675  loss: tensor(8897242., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1676  loss: tensor(4073385.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1677  loss: tensor(2961682.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1678  loss: tensor(1514887.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1679  loss: tensor(1222090.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1680  loss: tensor(3605192., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1681  loss: tensor(2022984.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1682  loss: tensor(2999865.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1683  loss: tensor(1846710.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1684  loss: tensor(2837986., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1685  loss: tensor(950525.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1686  loss: tensor(482118.3438, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1687  loss: tensor(854811.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1688  loss: tensor(2445391.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1689  loss: tensor(3111386., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1690  loss: tensor(4514215., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1691  loss: tensor(1165852.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1692  loss: tensor(644241.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1693  loss: tensor(3200803.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1694  loss: tensor(1586929.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1695  loss: tensor(185907.0781, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1696  loss: tensor(1549338., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1697  loss: tensor(3245210.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1698  loss: tensor(746779.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1699  loss: tensor(1237135.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1700  loss: tensor(3955934.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1701  loss: tensor(3502681., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1702  loss: tensor(3246487.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1703  loss: tensor(1042913.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1704  loss: tensor(1647754.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1705  loss: tensor(3048316.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1706  loss: tensor(2073734.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1707  loss: tensor(2795606.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1708  loss: tensor(3644797., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1709  loss: tensor(1312937.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1710  loss: tensor(7571279., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1711  loss: tensor(1654028., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1712  loss: tensor(1147449.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1713  loss: tensor(2968587.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1714  loss: tensor(3549857.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1715  loss: tensor(571325.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1716  loss: tensor(1608166.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1717  loss: tensor(3474825.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1718  loss: tensor(4400377., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1719  loss: tensor(523521.4688, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1720  loss: tensor(7252669.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1721  loss: tensor(1876483.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1722  loss: tensor(265975.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1723  loss: tensor(827410.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1724  loss: tensor(1208273.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1725  loss: tensor(3992873.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1726  loss: tensor(2709078.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1727  loss: tensor(2582923.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1728  loss: tensor(1220247.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1729  loss: tensor(4826153., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1730  loss: tensor(1978585.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1731  loss: tensor(7624001., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1732  loss: tensor(7714898.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1733  loss: tensor(1021858.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1734  loss: tensor(983958.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1735  loss: tensor(1082017.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1736  loss: tensor(11923238., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1737  loss: tensor(4097027.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1738  loss: tensor(1411219.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1739  loss: tensor(8250964., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1740  loss: tensor(1828298., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1741  loss: tensor(844718.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1742  loss: tensor(1107096., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1743  loss: tensor(2718055.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1744  loss: tensor(1454878.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1745  loss: tensor(1397058.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1746  loss: tensor(4314631., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1747  loss: tensor(11937118., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1748  loss: tensor(962926.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1749  loss: tensor(1041832., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1750  loss: tensor(2284486.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1751  loss: tensor(4649177.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1752  loss: tensor(747238.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1753  loss: tensor(8639078., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1754  loss: tensor(302356.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1755  loss: tensor(1304262.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1756  loss: tensor(1077499.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1757  loss: tensor(2515776.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1758  loss: tensor(3157606.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1759  loss: tensor(996431.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1760  loss: tensor(909964.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1761  loss: tensor(1351073.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1762  loss: tensor(7279076., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1763  loss: tensor(1591104.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1764  loss: tensor(1301542.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1765  loss: tensor(2789880.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1766  loss: tensor(1141664.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1767  loss: tensor(1471447., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1768  loss: tensor(1580945.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1769  loss: tensor(544075.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1770  loss: tensor(2180971.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1771  loss: tensor(7327816., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1772  loss: tensor(2756529., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1773  loss: tensor(838750.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1774  loss: tensor(5318448., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1775  loss: tensor(2985904.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1776  loss: tensor(3162124., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1777  loss: tensor(1801943.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1778  loss: tensor(3533658.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1779  loss: tensor(1478923.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1780  loss: tensor(5083341., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1781  loss: tensor(752417.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1782  loss: tensor(723802.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1783  loss: tensor(920797.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1784  loss: tensor(1127051.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1785  loss: tensor(7185281., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1786  loss: tensor(4948905.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1787  loss: tensor(969268.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1788  loss: tensor(1748198.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1789  loss: tensor(474356.5938, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1790  loss: tensor(2115652.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1791  loss: tensor(3201292.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1792  loss: tensor(1281201.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1793  loss: tensor(1183010.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1794  loss: tensor(3970280.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1795  loss: tensor(2273170.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1796  loss: tensor(8130891.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1797  loss: tensor(949042.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1798  loss: tensor(912839.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1799  loss: tensor(2175874., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1800  loss: tensor(1173675.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1801  loss: tensor(3217749.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1802  loss: tensor(1455867.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1803  loss: tensor(1489176.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1804  loss: tensor(1799469., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1805  loss: tensor(2247118.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1806  loss: tensor(1213371.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1807  loss: tensor(18833810., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1808  loss: tensor(1439593.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1809  loss: tensor(5725674.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1810  loss: tensor(1758280.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1811  loss: tensor(1400708., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1812  loss: tensor(2505863.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1813  loss: tensor(1528334.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1814  loss: tensor(672296.5625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1815  loss: tensor(490407.5312, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1816  loss: tensor(330205.3438, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1817  loss: tensor(2633972., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1818  loss: tensor(3699608.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1819  loss: tensor(995125.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1820  loss: tensor(6755964., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1821  loss: tensor(5344494., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1822  loss: tensor(5134889., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1823  loss: tensor(5979027., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1824  loss: tensor(3650833.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1825  loss: tensor(5981795., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1826  loss: tensor(698763.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1827  loss: tensor(4155430.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1828  loss: tensor(144321.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1829  loss: tensor(1253079.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1830  loss: tensor(1453229.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1831  loss: tensor(1724203.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1832  loss: tensor(1339420.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1833  loss: tensor(1015560.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1834  loss: tensor(1275757., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1835  loss: tensor(911818.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1836  loss: tensor(1481368.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1837  loss: tensor(4697968., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1838  loss: tensor(2864370., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1839  loss: tensor(1739169.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1840  loss: tensor(370077.0312, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1841  loss: tensor(415347.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1842  loss: tensor(8218610., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1843  loss: tensor(417732.0938, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1844  loss: tensor(1819912.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1845  loss: tensor(1111155.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1846  loss: tensor(1820976., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1847  loss: tensor(1863354.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1848  loss: tensor(681472., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1849  loss: tensor(1628295., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1850  loss: tensor(6948174., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1851  loss: tensor(4774931.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1852  loss: tensor(2305580., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1853  loss: tensor(4803391., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1854  loss: tensor(2833674., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1855  loss: tensor(2237727.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1856  loss: tensor(1492597.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1857  loss: tensor(3112313.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1858  loss: tensor(538850.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1859  loss: tensor(2029915.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1860  loss: tensor(1281101.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1861  loss: tensor(2325581.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1862  loss: tensor(3491280.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1863  loss: tensor(1387034.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1864  loss: tensor(1003400.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1865  loss: tensor(5718910., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1866  loss: tensor(2369558.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1867  loss: tensor(8172135.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1868  loss: tensor(1247651.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1869  loss: tensor(1074136.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1870  loss: tensor(4584425.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1871  loss: tensor(2540415.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1872  loss: tensor(328043.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1873  loss: tensor(2475443.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1874  loss: tensor(1412752.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1875  loss: tensor(693137.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1876  loss: tensor(7973269.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1877  loss: tensor(404800.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1878  loss: tensor(2133081.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1879  loss: tensor(913705.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1880  loss: tensor(1949174.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1881  loss: tensor(8246423., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1882  loss: tensor(537024.3125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1883  loss: tensor(7215266., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1884  loss: tensor(548600., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1885  loss: tensor(368489.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1886  loss: tensor(4428669., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1887  loss: tensor(363267.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1888  loss: tensor(6392189., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1889  loss: tensor(1649755.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1890  loss: tensor(2186953., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1891  loss: tensor(2876447.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1892  loss: tensor(2146189., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1893  loss: tensor(1775223.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1894  loss: tensor(2147022., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1895  loss: tensor(6304153.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1896  loss: tensor(1475502.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1897  loss: tensor(3390182., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1898  loss: tensor(2336731.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1899  loss: tensor(10189920., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1900  loss: tensor(877301.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1901  loss: tensor(5476350.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1902  loss: tensor(1571824.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1903  loss: tensor(700382.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1904  loss: tensor(618900., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1905  loss: tensor(1804876.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1906  loss: tensor(935443.4375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1907  loss: tensor(571426.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1908  loss: tensor(2170234., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1909  loss: tensor(1352863.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1910  loss: tensor(866856.1875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1911  loss: tensor(494178.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1912  loss: tensor(839970.8125, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1913  loss: tensor(1711489.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1914  loss: tensor(3216127., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1915  loss: tensor(1426780.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1916  loss: tensor(1344518., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1917  loss: tensor(5311768., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1918  loss: tensor(1261332.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1919  loss: tensor(2569742., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1920  loss: tensor(1220285.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1921  loss: tensor(4386245.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1922  loss: tensor(481063.0312, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1923  loss: tensor(560297.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1924  loss: tensor(1453185.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1925  loss: tensor(2627793., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1926  loss: tensor(2426169.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1927  loss: tensor(1200640.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1928  loss: tensor(5712331.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1929  loss: tensor(1857330.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1930  loss: tensor(4825279., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1931  loss: tensor(7756501., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1932  loss: tensor(738376.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1933  loss: tensor(5575789., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1934  loss: tensor(1083787.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1935  loss: tensor(1559078.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1936  loss: tensor(6399964., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1937  loss: tensor(14858882., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1938  loss: tensor(1300759.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1939  loss: tensor(1878411.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1940  loss: tensor(1460840.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1941  loss: tensor(287350.4688, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1942  loss: tensor(7974243., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1943  loss: tensor(1524026.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1944  loss: tensor(5806097.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1945  loss: tensor(1689161.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1946  loss: tensor(2475633.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1947  loss: tensor(2972343.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1948  loss: tensor(723555.6875, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1949  loss: tensor(2102445.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1950  loss: tensor(1814194.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1951  loss: tensor(1308775., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1952  loss: tensor(682227.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1953  loss: tensor(1608496.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1954  loss: tensor(1479292.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1955  loss: tensor(9190580., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1956  loss: tensor(1966196.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1957  loss: tensor(569507.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1958  loss: tensor(1239781.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1959  loss: tensor(1201810.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1960  loss: tensor(1667952.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1961  loss: tensor(1198757.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1962  loss: tensor(1491681.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1963  loss: tensor(1666531., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1964  loss: tensor(1777665.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1965  loss: tensor(1376325.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1966  loss: tensor(2586154.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1967  loss: tensor(4486441., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1968  loss: tensor(2484959., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1969  loss: tensor(560377.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1970  loss: tensor(1140974.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1971  loss: tensor(1805958.8750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1972  loss: tensor(1390821.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1973  loss: tensor(1611601.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1974  loss: tensor(11421734., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1975  loss: tensor(3202212.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1976  loss: tensor(8472775., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1977  loss: tensor(999577.0625, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1978  loss: tensor(1359398.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1979  loss: tensor(1473990.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1980  loss: tensor(744841.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1981  loss: tensor(10081273., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1982  loss: tensor(3207685.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1983  loss: tensor(823498.9375, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1984  loss: tensor(1339879.6250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1985  loss: tensor(1646387.3750, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1986  loss: tensor(3076497., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1987  loss: tensor(6567748.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1988  loss: tensor(1729755.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1989  loss: tensor(1159672.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1990  loss: tensor(3014924.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1991  loss: tensor(7556850., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1992  loss: tensor(2535141., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1993  loss: tensor(2255033.2500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1994  loss: tensor(1555225.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1995  loss: tensor(5941554., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1996  loss: tensor(961417.5000, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1997  loss: tensor(3752859.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1998  loss: tensor(3349727.7500, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 1999  loss: tensor(901907., grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 2000  loss: tensor(1979596.1250, grad_fn=<MseLossBackward0>)  lr:  [0.0001]\n",
      "step 2001  loss: tensor(2336470.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2002  loss: tensor(1471742.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2003  loss: tensor(1504084.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2004  loss: tensor(1854515.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2005  loss: tensor(2254198.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2006  loss: tensor(4208925., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2007  loss: tensor(9528392., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2008  loss: tensor(5758745., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2009  loss: tensor(2154026.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2010  loss: tensor(5286785.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2011  loss: tensor(2583576.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2012  loss: tensor(5428704., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2013  loss: tensor(3760104., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2014  loss: tensor(4919628.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2015  loss: tensor(5385484.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2016  loss: tensor(1158727.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2017  loss: tensor(1595010.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2018  loss: tensor(2107180.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2019  loss: tensor(1084852.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2020  loss: tensor(1657878., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2021  loss: tensor(2554389.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2022  loss: tensor(1314382.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2023  loss: tensor(6558999., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2024  loss: tensor(867462.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2025  loss: tensor(633563.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2026  loss: tensor(1370148.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2027  loss: tensor(1297374., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2028  loss: tensor(2159622.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2029  loss: tensor(1647781.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2030  loss: tensor(1777623.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2031  loss: tensor(4882186.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2032  loss: tensor(9976102., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2033  loss: tensor(1270667., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2034  loss: tensor(3796020.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2035  loss: tensor(2669315.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2036  loss: tensor(1012347.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2037  loss: tensor(1069053.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2038  loss: tensor(1306643.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2039  loss: tensor(2115805.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2040  loss: tensor(1278393., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2041  loss: tensor(487507.0312, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2042  loss: tensor(13240302., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2043  loss: tensor(1472602.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2044  loss: tensor(247122.3594, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2045  loss: tensor(880540.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2046  loss: tensor(3866014.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2047  loss: tensor(7001449., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2048  loss: tensor(1018923.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2049  loss: tensor(10746798., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2050  loss: tensor(5575132., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2051  loss: tensor(1719075., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2052  loss: tensor(3761995.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2053  loss: tensor(4695364., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2054  loss: tensor(1800166.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2055  loss: tensor(1905882., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2056  loss: tensor(1937497.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2057  loss: tensor(3175994.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2058  loss: tensor(2305655.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2059  loss: tensor(1755335.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2060  loss: tensor(4856020., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2061  loss: tensor(5570955.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2062  loss: tensor(1840883.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2063  loss: tensor(2507576.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2064  loss: tensor(3764474., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2065  loss: tensor(2220447.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2066  loss: tensor(1526115.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2067  loss: tensor(1477476., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2068  loss: tensor(2723363., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2069  loss: tensor(5550405., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2070  loss: tensor(8999845., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2071  loss: tensor(924969.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2072  loss: tensor(1353072., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2073  loss: tensor(1078210., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2074  loss: tensor(2014153.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2075  loss: tensor(941886.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2076  loss: tensor(2007962.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2077  loss: tensor(2856627.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2078  loss: tensor(1516168.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2079  loss: tensor(1515290.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2080  loss: tensor(1306208.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2081  loss: tensor(2096054.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2082  loss: tensor(2645169.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2083  loss: tensor(3306558.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2084  loss: tensor(1041093.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2085  loss: tensor(1215249.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2086  loss: tensor(900458.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2087  loss: tensor(1560774.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2088  loss: tensor(1035996.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2089  loss: tensor(995035.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2090  loss: tensor(840936.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2091  loss: tensor(1229664.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2092  loss: tensor(3221462., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2093  loss: tensor(1143139.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2094  loss: tensor(1356339.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2095  loss: tensor(609498.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2096  loss: tensor(1481989.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2097  loss: tensor(3884109.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2098  loss: tensor(3126987.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2099  loss: tensor(1993797.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2100  loss: tensor(2186672.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2101  loss: tensor(3506886., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2102  loss: tensor(1358834.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2103  loss: tensor(1463332.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2104  loss: tensor(719330., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2105  loss: tensor(1413101., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2106  loss: tensor(683996.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2107  loss: tensor(1781255., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2108  loss: tensor(6749491.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2109  loss: tensor(2536832.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2110  loss: tensor(11496073., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2111  loss: tensor(4087641.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2112  loss: tensor(1498991.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2113  loss: tensor(1171155.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2114  loss: tensor(1299206.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2115  loss: tensor(2063642.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2116  loss: tensor(2841223.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2117  loss: tensor(1862498.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2118  loss: tensor(2103234., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2119  loss: tensor(1815729.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2120  loss: tensor(1765301.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2121  loss: tensor(624801.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2122  loss: tensor(2908459.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2123  loss: tensor(1420025.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2124  loss: tensor(1855983.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2125  loss: tensor(888841.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2126  loss: tensor(1236965.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2127  loss: tensor(1552749.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2128  loss: tensor(2614538.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2129  loss: tensor(3575875.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2130  loss: tensor(1380049.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2131  loss: tensor(2873694., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2132  loss: tensor(819315.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2133  loss: tensor(1832952.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2134  loss: tensor(4182394.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2135  loss: tensor(6897936.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2136  loss: tensor(1314039., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2137  loss: tensor(2030936.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2138  loss: tensor(7726081., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2139  loss: tensor(580659.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2140  loss: tensor(11957005., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2141  loss: tensor(6608702., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2142  loss: tensor(3805143.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2143  loss: tensor(393117.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2144  loss: tensor(2259660.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2145  loss: tensor(1163065.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2146  loss: tensor(5498479., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2147  loss: tensor(2047541., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2148  loss: tensor(3294417.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2149  loss: tensor(814784.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2150  loss: tensor(3796808.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2151  loss: tensor(1765182.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2152  loss: tensor(783204.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2153  loss: tensor(3562042.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2154  loss: tensor(1615123.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2155  loss: tensor(615213.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2156  loss: tensor(1943810.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2157  loss: tensor(1224699.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2158  loss: tensor(241537.8906, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2159  loss: tensor(6299298., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2160  loss: tensor(3006728., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2161  loss: tensor(3140717.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2162  loss: tensor(619567.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2163  loss: tensor(3305231.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2164  loss: tensor(5303964., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2165  loss: tensor(2286384., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2166  loss: tensor(611801.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2167  loss: tensor(993240.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2168  loss: tensor(769546.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2169  loss: tensor(1189491.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2170  loss: tensor(945221.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2171  loss: tensor(5224889.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2172  loss: tensor(15969849., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2173  loss: tensor(1535109.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2174  loss: tensor(1917586.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2175  loss: tensor(1318858.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2176  loss: tensor(2151558.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2177  loss: tensor(2797533.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2178  loss: tensor(1602313.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2179  loss: tensor(7138143.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2180  loss: tensor(638690.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2181  loss: tensor(2617967.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2182  loss: tensor(1304858.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2183  loss: tensor(853809.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2184  loss: tensor(1206387.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2185  loss: tensor(1043939.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2186  loss: tensor(2931044., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2187  loss: tensor(2919631.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2188  loss: tensor(570711.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2189  loss: tensor(2217357.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2190  loss: tensor(2802540., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2191  loss: tensor(1702990.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2192  loss: tensor(4001632.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2193  loss: tensor(3125618.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2194  loss: tensor(1053266.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2195  loss: tensor(5634114.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2196  loss: tensor(1019601.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2197  loss: tensor(1390156.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2198  loss: tensor(2877566.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2199  loss: tensor(998643.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2200  loss: tensor(1730531.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2201  loss: tensor(304646., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2202  loss: tensor(3476345.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2203  loss: tensor(7274096., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2204  loss: tensor(9838100., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2205  loss: tensor(777497.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2206  loss: tensor(3396251.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2207  loss: tensor(10157176., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2208  loss: tensor(3377364.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2209  loss: tensor(1354080.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2210  loss: tensor(1680230.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2211  loss: tensor(1825701., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2212  loss: tensor(155823.2031, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2213  loss: tensor(4745326., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2214  loss: tensor(1387294.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2215  loss: tensor(1043875.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2216  loss: tensor(1002626.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2217  loss: tensor(14882666., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2218  loss: tensor(806704.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2219  loss: tensor(3788732.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2220  loss: tensor(1090736.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2221  loss: tensor(1104576.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2222  loss: tensor(543684.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2223  loss: tensor(627742.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2224  loss: tensor(2008297.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2225  loss: tensor(1930267., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2226  loss: tensor(4300899., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2227  loss: tensor(5756349., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2228  loss: tensor(5379888.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2229  loss: tensor(2548315., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2230  loss: tensor(1078917.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2231  loss: tensor(1671787.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2232  loss: tensor(3969655.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2233  loss: tensor(2229507.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2234  loss: tensor(2067339.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2235  loss: tensor(1274090., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2236  loss: tensor(1998796.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2237  loss: tensor(850338.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2238  loss: tensor(1435634.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2239  loss: tensor(372750.0938, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2240  loss: tensor(2083991.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2241  loss: tensor(2747881.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2242  loss: tensor(1890760.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2243  loss: tensor(1155899.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2244  loss: tensor(1500504.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2245  loss: tensor(657238., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2246  loss: tensor(1185991.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2247  loss: tensor(862626.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2248  loss: tensor(2246364., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2249  loss: tensor(564720.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2250  loss: tensor(585386.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2251  loss: tensor(5170078., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2252  loss: tensor(1319477., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2253  loss: tensor(4272314., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2254  loss: tensor(902103.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2255  loss: tensor(2470176.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2256  loss: tensor(756902.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2257  loss: tensor(604985.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2258  loss: tensor(2528059., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2259  loss: tensor(2458189.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2260  loss: tensor(2667760.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2261  loss: tensor(6337130., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2262  loss: tensor(1763966.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2263  loss: tensor(1759001.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2264  loss: tensor(3953161.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2265  loss: tensor(2168357.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2266  loss: tensor(4686402.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2267  loss: tensor(3809232., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2268  loss: tensor(1515076.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2269  loss: tensor(311431., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2270  loss: tensor(8405294., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2271  loss: tensor(698378.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2272  loss: tensor(1021672.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2273  loss: tensor(1509432.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2274  loss: tensor(2763752.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2275  loss: tensor(1078953., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2276  loss: tensor(748518.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2277  loss: tensor(2998123., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2278  loss: tensor(1078185.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2279  loss: tensor(2743971.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2280  loss: tensor(836097.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2281  loss: tensor(711308.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2282  loss: tensor(3249693.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2283  loss: tensor(6501978.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2284  loss: tensor(2089177., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2285  loss: tensor(1121002.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2286  loss: tensor(8831239., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2287  loss: tensor(2576897., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2288  loss: tensor(2256487.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2289  loss: tensor(1158252.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2290  loss: tensor(4750583., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2291  loss: tensor(3917100., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2292  loss: tensor(4933868., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2293  loss: tensor(3708464.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2294  loss: tensor(2072736.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2295  loss: tensor(595771.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2296  loss: tensor(846395.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2297  loss: tensor(2203913.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2298  loss: tensor(2823178.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2299  loss: tensor(2472290.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2300  loss: tensor(3402569.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2301  loss: tensor(1720980., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2302  loss: tensor(6109680., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2303  loss: tensor(3155920.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2304  loss: tensor(995604.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2305  loss: tensor(1561091.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2306  loss: tensor(2491880., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2307  loss: tensor(2845149.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2308  loss: tensor(4833959., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2309  loss: tensor(2648994.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2310  loss: tensor(2410097., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2311  loss: tensor(1027451.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2312  loss: tensor(2922255., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2313  loss: tensor(1541760.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2314  loss: tensor(1988470.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2315  loss: tensor(4645959., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2316  loss: tensor(3139670.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2317  loss: tensor(3389004.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2318  loss: tensor(5212835.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2319  loss: tensor(2860856.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2320  loss: tensor(1415563., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2321  loss: tensor(1703917.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2322  loss: tensor(4844543.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2323  loss: tensor(1107292., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2324  loss: tensor(1367446.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2325  loss: tensor(478874.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2326  loss: tensor(515796.5312, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2327  loss: tensor(3794234., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2328  loss: tensor(5139801., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2329  loss: tensor(694188.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2330  loss: tensor(4966168.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2331  loss: tensor(2752529., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2332  loss: tensor(1235901.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2333  loss: tensor(453344.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2334  loss: tensor(885467.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2335  loss: tensor(1588624.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2336  loss: tensor(2005826.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2337  loss: tensor(1371761.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2338  loss: tensor(2324860.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2339  loss: tensor(522838.2188, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2340  loss: tensor(4745087.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2341  loss: tensor(5539765.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2342  loss: tensor(863302.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2343  loss: tensor(998032.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2344  loss: tensor(718235.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2345  loss: tensor(1314434.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2346  loss: tensor(2692275., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2347  loss: tensor(984847.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2348  loss: tensor(2879575.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2349  loss: tensor(3183289.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2350  loss: tensor(4594921.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2351  loss: tensor(853953.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2352  loss: tensor(409354.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2353  loss: tensor(5657739.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2354  loss: tensor(1487538.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2355  loss: tensor(2104596.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2356  loss: tensor(778522.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2357  loss: tensor(3464388.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2358  loss: tensor(4302714., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2359  loss: tensor(6154136., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2360  loss: tensor(2566052.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2361  loss: tensor(1391593.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2362  loss: tensor(6117569., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2363  loss: tensor(1693726.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2364  loss: tensor(2133521.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2365  loss: tensor(1187300.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2366  loss: tensor(2048224.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2367  loss: tensor(5454175., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2368  loss: tensor(2228818.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2369  loss: tensor(1846866.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2370  loss: tensor(211811.4062, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2371  loss: tensor(3887652.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2372  loss: tensor(2055679.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2373  loss: tensor(3390261.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2374  loss: tensor(1110323.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2375  loss: tensor(2610270.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2376  loss: tensor(812564.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2377  loss: tensor(3460256., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2378  loss: tensor(2337428.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2379  loss: tensor(1372586.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2380  loss: tensor(694756.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2381  loss: tensor(3127546.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2382  loss: tensor(2728040.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2383  loss: tensor(1105961.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2384  loss: tensor(962878.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2385  loss: tensor(3634722.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2386  loss: tensor(3006346.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2387  loss: tensor(2239712., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2388  loss: tensor(1146456.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2389  loss: tensor(1713346.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2390  loss: tensor(5075972.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2391  loss: tensor(5297799.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2392  loss: tensor(2943161., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2393  loss: tensor(1591292.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2394  loss: tensor(1369954., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2395  loss: tensor(739349., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2396  loss: tensor(5211849., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2397  loss: tensor(3543475.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2398  loss: tensor(4353351.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2399  loss: tensor(1289768., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2400  loss: tensor(2246999., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2401  loss: tensor(4957343., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2402  loss: tensor(1828572., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2403  loss: tensor(3337496.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2404  loss: tensor(854352., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2405  loss: tensor(3614046.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2406  loss: tensor(2167385.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2407  loss: tensor(12725038., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2408  loss: tensor(4711374., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2409  loss: tensor(3080432.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2410  loss: tensor(1658610.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2411  loss: tensor(3173807., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2412  loss: tensor(515450.8438, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2413  loss: tensor(1665282.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2414  loss: tensor(2921031.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2415  loss: tensor(970411.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2416  loss: tensor(1345858.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2417  loss: tensor(1079952.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2418  loss: tensor(1249873.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2419  loss: tensor(3298045.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2420  loss: tensor(896273.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2421  loss: tensor(1719822.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2422  loss: tensor(1436156., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2423  loss: tensor(1078057.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2424  loss: tensor(1058145.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2425  loss: tensor(1382833., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2426  loss: tensor(1226839.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2427  loss: tensor(1217786.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2428  loss: tensor(1497688., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2429  loss: tensor(3558437.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2430  loss: tensor(3018628.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2431  loss: tensor(1513851.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2432  loss: tensor(2076404.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2433  loss: tensor(2867228.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2434  loss: tensor(1598710.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2435  loss: tensor(1387602.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2436  loss: tensor(1050725.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2437  loss: tensor(16261096., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2438  loss: tensor(3027411.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2439  loss: tensor(1217706.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2440  loss: tensor(2646840., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2441  loss: tensor(1429949.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2442  loss: tensor(1762495.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2443  loss: tensor(1918598.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2444  loss: tensor(1631152.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2445  loss: tensor(1176122.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2446  loss: tensor(3350904.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2447  loss: tensor(2546462.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2448  loss: tensor(1204666.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2449  loss: tensor(4266352.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2450  loss: tensor(2713553.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2451  loss: tensor(3445625.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2452  loss: tensor(2848454.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2453  loss: tensor(2439477.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2454  loss: tensor(4575197., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2455  loss: tensor(885085.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2456  loss: tensor(1147113.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2457  loss: tensor(2066352.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2458  loss: tensor(1415254.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2459  loss: tensor(557407.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2460  loss: tensor(9878610., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2461  loss: tensor(1293513.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2462  loss: tensor(2193553.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2463  loss: tensor(863526.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2464  loss: tensor(1845041.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2465  loss: tensor(11801407., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2466  loss: tensor(748078.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2467  loss: tensor(16474407., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2468  loss: tensor(2687201.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2469  loss: tensor(1378628.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2470  loss: tensor(14025626., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2471  loss: tensor(1928101.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2472  loss: tensor(4859215., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2473  loss: tensor(695003.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2474  loss: tensor(945741.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2475  loss: tensor(1964205.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2476  loss: tensor(1153357.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2477  loss: tensor(1306971.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2478  loss: tensor(1453454.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2479  loss: tensor(1701649.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2480  loss: tensor(1394592.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2481  loss: tensor(322925.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2482  loss: tensor(4496160.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2483  loss: tensor(1459485.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2484  loss: tensor(5872288., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2485  loss: tensor(1819837.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2486  loss: tensor(3196354.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2487  loss: tensor(1583698.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2488  loss: tensor(1102399., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2489  loss: tensor(635010.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2490  loss: tensor(1375296.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2491  loss: tensor(1449707.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2492  loss: tensor(328009.9062, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2493  loss: tensor(1347811.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2494  loss: tensor(2946392.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2495  loss: tensor(1872848.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2496  loss: tensor(2538867.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2497  loss: tensor(2042942.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2498  loss: tensor(4231534., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2499  loss: tensor(2147967.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2500  loss: tensor(1854942.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2501  loss: tensor(546727.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2502  loss: tensor(1042984.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2503  loss: tensor(4410018., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2504  loss: tensor(3342751.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2505  loss: tensor(1428171., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2506  loss: tensor(2234983.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2507  loss: tensor(1641437.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2508  loss: tensor(343802.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2509  loss: tensor(895203.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2510  loss: tensor(3649230., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2511  loss: tensor(2561832., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2512  loss: tensor(2985107.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2513  loss: tensor(389624.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2514  loss: tensor(1338564.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2515  loss: tensor(688833.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2516  loss: tensor(2796421.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2517  loss: tensor(846360.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2518  loss: tensor(809837.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2519  loss: tensor(647025.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2520  loss: tensor(2577677.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2521  loss: tensor(3542550., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2522  loss: tensor(1349921.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2523  loss: tensor(4256177.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2524  loss: tensor(800867.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2525  loss: tensor(398596.4688, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2526  loss: tensor(1192676.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2527  loss: tensor(904545.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2528  loss: tensor(1729474.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2529  loss: tensor(4188986.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2530  loss: tensor(3221843.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2531  loss: tensor(3675616., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2532  loss: tensor(1761675.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2533  loss: tensor(1838812.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2534  loss: tensor(7439788., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2535  loss: tensor(519940.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2536  loss: tensor(570734., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2537  loss: tensor(760789.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2538  loss: tensor(2931440., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2539  loss: tensor(2567274.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2540  loss: tensor(2031789., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2541  loss: tensor(3430785.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2542  loss: tensor(3478489.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2543  loss: tensor(2755995.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2544  loss: tensor(2464192., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2545  loss: tensor(2357122.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2546  loss: tensor(1331391.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2547  loss: tensor(2663031.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2548  loss: tensor(3724842., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2549  loss: tensor(771894.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2550  loss: tensor(2634095.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2551  loss: tensor(3947749., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2552  loss: tensor(1233159.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2553  loss: tensor(375270.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2554  loss: tensor(1363686.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2555  loss: tensor(1103132., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2556  loss: tensor(401030.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2557  loss: tensor(3335173.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2558  loss: tensor(2211208., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2559  loss: tensor(1669670.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2560  loss: tensor(1599065., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2561  loss: tensor(449602.0312, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2562  loss: tensor(578279.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2563  loss: tensor(4103561.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2564  loss: tensor(1102629.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2565  loss: tensor(474687.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2566  loss: tensor(2879000.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2567  loss: tensor(1022110., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2568  loss: tensor(4197818.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2569  loss: tensor(2722455.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2570  loss: tensor(2966801., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2571  loss: tensor(796537.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2572  loss: tensor(2272819.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2573  loss: tensor(1024462.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2574  loss: tensor(1272283.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2575  loss: tensor(579798.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2576  loss: tensor(1417795.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2577  loss: tensor(2492240.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2578  loss: tensor(6813032.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2579  loss: tensor(3654445.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2580  loss: tensor(1333633.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2581  loss: tensor(1319690.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2582  loss: tensor(1111864.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2583  loss: tensor(1972976.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2584  loss: tensor(1848445.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2585  loss: tensor(2258904.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2586  loss: tensor(1767470., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2587  loss: tensor(1270591.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2588  loss: tensor(3640358.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2589  loss: tensor(7569871.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2590  loss: tensor(1701420.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2591  loss: tensor(705712.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2592  loss: tensor(3023105.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2593  loss: tensor(2244110.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2594  loss: tensor(1533122.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2595  loss: tensor(3744442.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2596  loss: tensor(3218560.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2597  loss: tensor(4968735.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2598  loss: tensor(1700723., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2599  loss: tensor(1968636.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2600  loss: tensor(2574470.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2601  loss: tensor(2910240.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2602  loss: tensor(9232124., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2603  loss: tensor(2206039., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2604  loss: tensor(1263592.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2605  loss: tensor(516683.0312, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2606  loss: tensor(1742370.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2607  loss: tensor(8101128., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2608  loss: tensor(1795117.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2609  loss: tensor(1378433.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2610  loss: tensor(3009617.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2611  loss: tensor(811290.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2612  loss: tensor(3141188.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2613  loss: tensor(2086816.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2614  loss: tensor(544703.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2615  loss: tensor(2243274.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2616  loss: tensor(1543390., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2617  loss: tensor(6873637.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2618  loss: tensor(997858.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2619  loss: tensor(909936.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2620  loss: tensor(1052184.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2621  loss: tensor(1255870.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2622  loss: tensor(3057666.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2623  loss: tensor(3983835., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2624  loss: tensor(2643375.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2625  loss: tensor(856307.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2626  loss: tensor(1699126.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2627  loss: tensor(692990.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2628  loss: tensor(1678010.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2629  loss: tensor(2568419., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2630  loss: tensor(2706840., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2631  loss: tensor(771672.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2632  loss: tensor(2285857.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2633  loss: tensor(1001103.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2634  loss: tensor(1107266.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2635  loss: tensor(6584583., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2636  loss: tensor(5427222., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2637  loss: tensor(8755127., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2638  loss: tensor(2873021.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2639  loss: tensor(1779403.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2640  loss: tensor(1762939.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2641  loss: tensor(2119941.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2642  loss: tensor(2289257.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2643  loss: tensor(392877.0938, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2644  loss: tensor(723702.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2645  loss: tensor(2001025.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2646  loss: tensor(210637.0938, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2647  loss: tensor(3013811.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2648  loss: tensor(2636501., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2649  loss: tensor(1188756.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2650  loss: tensor(2893383., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2651  loss: tensor(3965595.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2652  loss: tensor(2466886.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2653  loss: tensor(2645052.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2654  loss: tensor(740460.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2655  loss: tensor(1511697.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2656  loss: tensor(2968583., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2657  loss: tensor(8835767., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2658  loss: tensor(930424.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2659  loss: tensor(979691.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2660  loss: tensor(1388961.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2661  loss: tensor(20240228., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2662  loss: tensor(2558702.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2663  loss: tensor(2756074., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2664  loss: tensor(992369., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2665  loss: tensor(7504816.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2666  loss: tensor(2707597., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2667  loss: tensor(3130026.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2668  loss: tensor(3912649.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2669  loss: tensor(2268413.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2670  loss: tensor(315483.4062, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2671  loss: tensor(1284331.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2672  loss: tensor(6865428., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2673  loss: tensor(3642603.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2674  loss: tensor(2345814.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2675  loss: tensor(1962581.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2676  loss: tensor(4467177., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2677  loss: tensor(2250853.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2678  loss: tensor(1389825., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2679  loss: tensor(929266.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2680  loss: tensor(13498801., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2681  loss: tensor(8042767.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2682  loss: tensor(1658197., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2683  loss: tensor(3841375., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2684  loss: tensor(6448766.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2685  loss: tensor(785538.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2686  loss: tensor(3221162.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2687  loss: tensor(1122267.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2688  loss: tensor(2059984.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2689  loss: tensor(5781947., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2690  loss: tensor(834594.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2691  loss: tensor(769039.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2692  loss: tensor(1491526.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2693  loss: tensor(3025110.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2694  loss: tensor(1322771.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2695  loss: tensor(2499801.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2696  loss: tensor(1820949.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2697  loss: tensor(982596.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2698  loss: tensor(1337601., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2699  loss: tensor(1307630., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2700  loss: tensor(1438345.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2701  loss: tensor(3880404.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2702  loss: tensor(794514.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2703  loss: tensor(2315387.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2704  loss: tensor(803346.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2705  loss: tensor(1092193.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2706  loss: tensor(3232311., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2707  loss: tensor(2550076., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2708  loss: tensor(625507.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2709  loss: tensor(664110.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2710  loss: tensor(487981.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2711  loss: tensor(1968484.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2712  loss: tensor(2438141.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2713  loss: tensor(667133.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2714  loss: tensor(1713688.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2715  loss: tensor(2910970.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2716  loss: tensor(1795580.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2717  loss: tensor(1271587.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2718  loss: tensor(2085893.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2719  loss: tensor(2105775.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2720  loss: tensor(4983000., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2721  loss: tensor(1798840.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2722  loss: tensor(810693.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2723  loss: tensor(1748709.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2724  loss: tensor(1906282., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2725  loss: tensor(1557170.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2726  loss: tensor(4262082.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2727  loss: tensor(23639326., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2728  loss: tensor(7109879., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2729  loss: tensor(9631458., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2730  loss: tensor(673144.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2731  loss: tensor(2298466.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2732  loss: tensor(1776720.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2733  loss: tensor(675562.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2734  loss: tensor(618656.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2735  loss: tensor(614033.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2736  loss: tensor(2454461., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2737  loss: tensor(1332231., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2738  loss: tensor(2593947., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2739  loss: tensor(7222765., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2740  loss: tensor(2416234., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2741  loss: tensor(1309872., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2742  loss: tensor(840416.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2743  loss: tensor(2519356., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2744  loss: tensor(930349.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2745  loss: tensor(802706.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2746  loss: tensor(6125473.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2747  loss: tensor(547599.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2748  loss: tensor(1145340.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2749  loss: tensor(3611020.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2750  loss: tensor(2702237.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2751  loss: tensor(1775688., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2752  loss: tensor(3556797.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2753  loss: tensor(2665155.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2754  loss: tensor(2377625., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2755  loss: tensor(2319559.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2756  loss: tensor(1732753.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2757  loss: tensor(1455627.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2758  loss: tensor(1494919., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2759  loss: tensor(1857780., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2760  loss: tensor(3049270.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2761  loss: tensor(801678.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2762  loss: tensor(1646914.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2763  loss: tensor(2943030., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2764  loss: tensor(327624.2812, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2765  loss: tensor(2277308., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2766  loss: tensor(377684.1562, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2767  loss: tensor(4153985.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2768  loss: tensor(1170756., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2769  loss: tensor(1217286.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2770  loss: tensor(3698334., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2771  loss: tensor(968607.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2772  loss: tensor(254744.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2773  loss: tensor(1620039.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2774  loss: tensor(1283149.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2775  loss: tensor(4595925.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2776  loss: tensor(1206358.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2777  loss: tensor(1588796.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2778  loss: tensor(6187737., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2779  loss: tensor(2815567.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2780  loss: tensor(1386164.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2781  loss: tensor(495863., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2782  loss: tensor(1984685.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2783  loss: tensor(896719.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2784  loss: tensor(1605573.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2785  loss: tensor(6680323., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2786  loss: tensor(3304148., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2787  loss: tensor(9390155., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2788  loss: tensor(1775229.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2789  loss: tensor(408365.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2790  loss: tensor(917352.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2791  loss: tensor(949527.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2792  loss: tensor(5405469.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2793  loss: tensor(2161752.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2794  loss: tensor(3865062., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2795  loss: tensor(1158288.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2796  loss: tensor(2687476.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2797  loss: tensor(1813588.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2798  loss: tensor(3871080.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2799  loss: tensor(589827.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2800  loss: tensor(1076972.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2801  loss: tensor(2835591.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2802  loss: tensor(1368274.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2803  loss: tensor(760225.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2804  loss: tensor(1252032.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2805  loss: tensor(2917795.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2806  loss: tensor(2008184., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2807  loss: tensor(779732.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2808  loss: tensor(10369767., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2809  loss: tensor(1212765.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2810  loss: tensor(3270094.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2811  loss: tensor(10119611., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2812  loss: tensor(24341194., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2813  loss: tensor(947615.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2814  loss: tensor(20253370., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2815  loss: tensor(1315959.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2816  loss: tensor(1346619.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2817  loss: tensor(1671271.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2818  loss: tensor(1953967.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2819  loss: tensor(1070862.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2820  loss: tensor(428082.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2821  loss: tensor(3414582., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2822  loss: tensor(1776991.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2823  loss: tensor(1733291.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2824  loss: tensor(5767930.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2825  loss: tensor(3777105.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2826  loss: tensor(1803674.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2827  loss: tensor(685643.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2828  loss: tensor(4857354., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2829  loss: tensor(8265568.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2830  loss: tensor(1322076., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2831  loss: tensor(605669.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2832  loss: tensor(3814074.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2833  loss: tensor(929167.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2834  loss: tensor(2380936.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2835  loss: tensor(2897398.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2836  loss: tensor(4056097.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2837  loss: tensor(852739.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2838  loss: tensor(3417342.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2839  loss: tensor(869496.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2840  loss: tensor(2155868.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2841  loss: tensor(2816244., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2842  loss: tensor(3014671., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2843  loss: tensor(1470141.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2844  loss: tensor(825847.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2845  loss: tensor(14991160., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2846  loss: tensor(4060462.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2847  loss: tensor(1554267., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2848  loss: tensor(621388.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2849  loss: tensor(2416725.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2850  loss: tensor(1105955.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2851  loss: tensor(515070.5938, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2852  loss: tensor(1828013.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2853  loss: tensor(1820981.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2854  loss: tensor(3110495.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2855  loss: tensor(3657455.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2856  loss: tensor(988466.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2857  loss: tensor(1206144.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2858  loss: tensor(1143883.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2859  loss: tensor(2562452.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2860  loss: tensor(1619540.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2861  loss: tensor(927382.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2862  loss: tensor(2750480.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2863  loss: tensor(4764266.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2864  loss: tensor(757686.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2865  loss: tensor(5559948.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2866  loss: tensor(5440702., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2867  loss: tensor(2458646., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2868  loss: tensor(1507721.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2869  loss: tensor(1511387.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2870  loss: tensor(933359.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2871  loss: tensor(2127161., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2872  loss: tensor(1159724.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2873  loss: tensor(2651599., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2874  loss: tensor(1835785.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2875  loss: tensor(4348778.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2876  loss: tensor(3053627., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2877  loss: tensor(526582.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2878  loss: tensor(1905682.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2879  loss: tensor(4097658.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2880  loss: tensor(687953.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2881  loss: tensor(3632979.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2882  loss: tensor(3020195.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2883  loss: tensor(2627904.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2884  loss: tensor(923420.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2885  loss: tensor(1634174.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2886  loss: tensor(945319.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2887  loss: tensor(1757693.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2888  loss: tensor(3307347., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2889  loss: tensor(927210.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2890  loss: tensor(322757.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2891  loss: tensor(1152707., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2892  loss: tensor(940103.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2893  loss: tensor(926873.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2894  loss: tensor(3552542., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2895  loss: tensor(2268765.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2896  loss: tensor(1333347.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2897  loss: tensor(1156165.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2898  loss: tensor(1048952.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2899  loss: tensor(2306357., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2900  loss: tensor(2234953.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2901  loss: tensor(1378587.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2902  loss: tensor(12234215., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2903  loss: tensor(2375643.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2904  loss: tensor(4209691., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2905  loss: tensor(1609832., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2906  loss: tensor(3306359., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2907  loss: tensor(3496294.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2908  loss: tensor(4823635.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2909  loss: tensor(1225961., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2910  loss: tensor(1724575.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2911  loss: tensor(722700.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2912  loss: tensor(1309386.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2913  loss: tensor(2892571.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2914  loss: tensor(590884.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2915  loss: tensor(1482346.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2916  loss: tensor(2187327., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2917  loss: tensor(4286009., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2918  loss: tensor(2624547.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2919  loss: tensor(1264431.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2920  loss: tensor(2864489.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2921  loss: tensor(477347.7188, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2922  loss: tensor(1114099.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2923  loss: tensor(13355376., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2924  loss: tensor(965570.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2925  loss: tensor(4836402., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2926  loss: tensor(1223651.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2927  loss: tensor(4008643.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2928  loss: tensor(2700726.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2929  loss: tensor(863500.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2930  loss: tensor(3443057.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2931  loss: tensor(1156443.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2932  loss: tensor(1843902.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2933  loss: tensor(3932164., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2934  loss: tensor(1600400.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2935  loss: tensor(2762381., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2936  loss: tensor(2087205.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2937  loss: tensor(2282092.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2938  loss: tensor(4297114.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2939  loss: tensor(257691.8438, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2940  loss: tensor(3027852.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2941  loss: tensor(7915620., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2942  loss: tensor(562100., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2943  loss: tensor(1606622.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2944  loss: tensor(1166475.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2945  loss: tensor(646765.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2946  loss: tensor(1745820.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2947  loss: tensor(3730939.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2948  loss: tensor(2240065.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2949  loss: tensor(1008869.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2950  loss: tensor(3688536.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2951  loss: tensor(1482996.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2952  loss: tensor(2022500.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2953  loss: tensor(3314106.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2954  loss: tensor(1694279., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2955  loss: tensor(689963.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2956  loss: tensor(6211596., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2957  loss: tensor(912553.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2958  loss: tensor(1658601.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2959  loss: tensor(1594989., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2960  loss: tensor(4728093.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2961  loss: tensor(155455., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2962  loss: tensor(10804778., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2963  loss: tensor(7591651., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2964  loss: tensor(4423597.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2965  loss: tensor(1628675.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2966  loss: tensor(3325313.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2967  loss: tensor(2905687.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2968  loss: tensor(357427., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2969  loss: tensor(1530161.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2970  loss: tensor(6501772., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2971  loss: tensor(2362916., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2972  loss: tensor(3204513.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2973  loss: tensor(2809607., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2974  loss: tensor(964398.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2975  loss: tensor(691578.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2976  loss: tensor(3067091., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2977  loss: tensor(2611695.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2978  loss: tensor(1335323.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2979  loss: tensor(2272258.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2980  loss: tensor(1235040., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2981  loss: tensor(6579435.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2982  loss: tensor(1094335.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2983  loss: tensor(7754033., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2984  loss: tensor(8465215., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2985  loss: tensor(2669866.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2986  loss: tensor(3453853., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2987  loss: tensor(6856634., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2988  loss: tensor(3630112., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2989  loss: tensor(1142217.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2990  loss: tensor(7967239., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2991  loss: tensor(1915644., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2992  loss: tensor(13565540., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2993  loss: tensor(1878709.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2994  loss: tensor(2971540.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2995  loss: tensor(306247.7188, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2996  loss: tensor(2807630., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2997  loss: tensor(1948594.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2998  loss: tensor(4206228., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 2999  loss: tensor(1763183.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3000  loss: tensor(1623489.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3001  loss: tensor(4600867.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3002  loss: tensor(884872.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3003  loss: tensor(11782970., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3004  loss: tensor(969968., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3005  loss: tensor(4020182., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3006  loss: tensor(2174326., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3007  loss: tensor(3935198., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3008  loss: tensor(2457072., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3009  loss: tensor(821942.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3010  loss: tensor(1435621.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3011  loss: tensor(1182122.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3012  loss: tensor(1776102.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3013  loss: tensor(1583098.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3014  loss: tensor(2122111.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3015  loss: tensor(2482988.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3016  loss: tensor(5374067., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3017  loss: tensor(921332.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3018  loss: tensor(3393898., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3019  loss: tensor(14215647., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3020  loss: tensor(4606047.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3021  loss: tensor(600843.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3022  loss: tensor(942537.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3023  loss: tensor(859385.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3024  loss: tensor(384177.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3025  loss: tensor(1675791.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3026  loss: tensor(11261189., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3027  loss: tensor(2397552.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3028  loss: tensor(536030.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3029  loss: tensor(3783525.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3030  loss: tensor(3753557., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3031  loss: tensor(8199917., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3032  loss: tensor(2152885.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3033  loss: tensor(1795115.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3034  loss: tensor(1701270.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3035  loss: tensor(2432394.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3036  loss: tensor(1456725.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3037  loss: tensor(1666366.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3038  loss: tensor(2084597.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3039  loss: tensor(3157627., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3040  loss: tensor(518494.7188, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3041  loss: tensor(2520013.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3042  loss: tensor(650486.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3043  loss: tensor(1348552.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3044  loss: tensor(5567592., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3045  loss: tensor(1227160., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3046  loss: tensor(600709.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3047  loss: tensor(18500192., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3048  loss: tensor(1457020., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3049  loss: tensor(2879229.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3050  loss: tensor(2529587.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3051  loss: tensor(2165245.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3052  loss: tensor(1091416.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3053  loss: tensor(905590.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3054  loss: tensor(1054498.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3055  loss: tensor(1684308.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3056  loss: tensor(1848506.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3057  loss: tensor(1363331.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3058  loss: tensor(1215393.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3059  loss: tensor(4989778., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3060  loss: tensor(1068984.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3061  loss: tensor(1022751.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3062  loss: tensor(1069506., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3063  loss: tensor(4962242., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3064  loss: tensor(4801838., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3065  loss: tensor(4038456.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3066  loss: tensor(2044946., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3067  loss: tensor(6324120., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3068  loss: tensor(658512.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3069  loss: tensor(13546444., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3070  loss: tensor(13821956., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3071  loss: tensor(5728338.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3072  loss: tensor(4533533., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3073  loss: tensor(5581017.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3074  loss: tensor(1425220.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3075  loss: tensor(757440.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3076  loss: tensor(977964.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3077  loss: tensor(1037582.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3078  loss: tensor(1647876.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3079  loss: tensor(1381481.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3080  loss: tensor(1351702., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3081  loss: tensor(5392795., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3082  loss: tensor(1312457.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3083  loss: tensor(902346.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3084  loss: tensor(2008999.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3085  loss: tensor(2456565.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3086  loss: tensor(2185158.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3087  loss: tensor(795027.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3088  loss: tensor(1017613.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3089  loss: tensor(1608708.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3090  loss: tensor(4646183., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3091  loss: tensor(3591612.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3092  loss: tensor(739439.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3093  loss: tensor(266508.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3094  loss: tensor(1241481.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3095  loss: tensor(2852256., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3096  loss: tensor(730928.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3097  loss: tensor(4881373., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3098  loss: tensor(3322781., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3099  loss: tensor(1395816., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3100  loss: tensor(311614.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3101  loss: tensor(4961023.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3102  loss: tensor(962889.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3103  loss: tensor(1200632.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3104  loss: tensor(1324064.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3105  loss: tensor(1325057.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3106  loss: tensor(1066134., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3107  loss: tensor(7627764., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3108  loss: tensor(1811599.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3109  loss: tensor(2070990.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3110  loss: tensor(291595.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3111  loss: tensor(2953699., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3112  loss: tensor(1146385., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3113  loss: tensor(1114197.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3114  loss: tensor(2445590., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3115  loss: tensor(482271.3438, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3116  loss: tensor(782669., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3117  loss: tensor(3795388.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3118  loss: tensor(3591100., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3119  loss: tensor(4338303., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3120  loss: tensor(10754398., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3121  loss: tensor(2346461.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3122  loss: tensor(5259474.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3123  loss: tensor(7156714., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3124  loss: tensor(5904784., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3125  loss: tensor(1285606.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3126  loss: tensor(2823570.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3127  loss: tensor(1922403.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3128  loss: tensor(2585235.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3129  loss: tensor(978037.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3130  loss: tensor(2845483., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3131  loss: tensor(3432390.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3132  loss: tensor(3587514., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3133  loss: tensor(2460523.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3134  loss: tensor(1257625.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3135  loss: tensor(2995676.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3136  loss: tensor(1168683.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3137  loss: tensor(3309538.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3138  loss: tensor(11255834., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3139  loss: tensor(829127.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3140  loss: tensor(3703932., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3141  loss: tensor(856970.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3142  loss: tensor(1130356.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3143  loss: tensor(2324302.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3144  loss: tensor(2484091.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3145  loss: tensor(2111278.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3146  loss: tensor(3193944.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3147  loss: tensor(9713150., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3148  loss: tensor(8468969., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3149  loss: tensor(2123844.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3150  loss: tensor(2581461.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3151  loss: tensor(1124770., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3152  loss: tensor(1882990.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3153  loss: tensor(866298.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3154  loss: tensor(6360806., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3155  loss: tensor(1188777.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3156  loss: tensor(5403566., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3157  loss: tensor(818654.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3158  loss: tensor(3480089.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3159  loss: tensor(805826.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3160  loss: tensor(6924418., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3161  loss: tensor(3047057.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3162  loss: tensor(1780575.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3163  loss: tensor(626479.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3164  loss: tensor(1082391., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3165  loss: tensor(1536757.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3166  loss: tensor(2226835., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3167  loss: tensor(2469746., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3168  loss: tensor(1878684.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3169  loss: tensor(11547136., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3170  loss: tensor(513928.4062, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3171  loss: tensor(3203631.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3172  loss: tensor(1407389.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3173  loss: tensor(3302769., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3174  loss: tensor(1435890.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3175  loss: tensor(662928., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3176  loss: tensor(6409274.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3177  loss: tensor(558562.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3178  loss: tensor(2685506.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3179  loss: tensor(625722.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3180  loss: tensor(1601388.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3181  loss: tensor(1271477.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3182  loss: tensor(485605.4688, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3183  loss: tensor(1275593.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3184  loss: tensor(4495164.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3185  loss: tensor(1380306.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3186  loss: tensor(7212948., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3187  loss: tensor(1069464.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3188  loss: tensor(2031634.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3189  loss: tensor(3233763.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3190  loss: tensor(2663246.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3191  loss: tensor(1481558.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3192  loss: tensor(2301849.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3193  loss: tensor(729838.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3194  loss: tensor(544076.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3195  loss: tensor(528818.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3196  loss: tensor(1220625.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3197  loss: tensor(1376010.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3198  loss: tensor(5506974., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3199  loss: tensor(5244205., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3200  loss: tensor(1871537.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3201  loss: tensor(1750604.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3202  loss: tensor(1360748.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3203  loss: tensor(1578264.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3204  loss: tensor(846309.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3205  loss: tensor(3148014.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3206  loss: tensor(4078966.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3207  loss: tensor(1784623.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3208  loss: tensor(582652.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3209  loss: tensor(1388602.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3210  loss: tensor(1591918.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3211  loss: tensor(643590.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3212  loss: tensor(7723384.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3213  loss: tensor(1974502.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3214  loss: tensor(1604078., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3215  loss: tensor(2346381.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3216  loss: tensor(1540052.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3217  loss: tensor(1115849., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3218  loss: tensor(3045570.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3219  loss: tensor(1429265.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3220  loss: tensor(682449.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3221  loss: tensor(2926324.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3222  loss: tensor(742107.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3223  loss: tensor(1228867., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3224  loss: tensor(918177.4375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3225  loss: tensor(965074.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3226  loss: tensor(507282.2812, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3227  loss: tensor(17337194., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3228  loss: tensor(4393309.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3229  loss: tensor(878514.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3230  loss: tensor(2225127.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3231  loss: tensor(1389577.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3232  loss: tensor(10122676., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3233  loss: tensor(4173487.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3234  loss: tensor(4068505., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3235  loss: tensor(1889217.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3236  loss: tensor(2105756.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3237  loss: tensor(1159570.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3238  loss: tensor(1704250.8750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3239  loss: tensor(2600666.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3240  loss: tensor(1911008., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3241  loss: tensor(489882.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3242  loss: tensor(4427514.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3243  loss: tensor(584036.8125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3244  loss: tensor(3080073.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3245  loss: tensor(5085057., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3246  loss: tensor(721006., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3247  loss: tensor(1489108.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3248  loss: tensor(1141866., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3249  loss: tensor(2226427.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3250  loss: tensor(1806867.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3251  loss: tensor(1351595.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3252  loss: tensor(2388242., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3253  loss: tensor(1453797., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3254  loss: tensor(1298787.6250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3255  loss: tensor(868558., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3256  loss: tensor(10610858., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3257  loss: tensor(769166.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3258  loss: tensor(758060.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3259  loss: tensor(2106992.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3260  loss: tensor(1633246.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3261  loss: tensor(2360461.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3262  loss: tensor(759926.1875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3263  loss: tensor(1047857.0625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3264  loss: tensor(2357787.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3265  loss: tensor(2365447.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3266  loss: tensor(976341.3125, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3267  loss: tensor(347461.6875, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3268  loss: tensor(2112253.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3269  loss: tensor(436967.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3270  loss: tensor(1495324.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3271  loss: tensor(1234649., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3272  loss: tensor(2192208.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3273  loss: tensor(1918984.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3274  loss: tensor(5163031.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3275  loss: tensor(1003671.9375, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3276  loss: tensor(4341530., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3277  loss: tensor(1625267.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3278  loss: tensor(1270674.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3279  loss: tensor(3132569.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3280  loss: tensor(1038891.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3281  loss: tensor(7457038., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3282  loss: tensor(2850737.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3283  loss: tensor(3805874.2500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3284  loss: tensor(1836129.1250, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3285  loss: tensor(12383738., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3286  loss: tensor(1505266.3750, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3287  loss: tensor(1892753.7500, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3288  loss: tensor(1912132.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3289  loss: tensor(495863.5625, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3290  loss: tensor(1177609., grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n",
      "step 3291  loss: tensor(6396890.5000, grad_fn=<MseLossBackward0>)  lr:  [9e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     14\u001B[0m t \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     15\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 16\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m step\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m,step,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m loss:\u001B[39m\u001B[38;5;124m'\u001B[39m,loss,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m lr: \u001B[39m\u001B[38;5;124m'\u001B[39m,lr_scheduler\u001B[38;5;241m.\u001B[39mget_last_lr())\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36mHGCN.forward\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    160\u001B[0m edge_mask \u001B[38;5;241m=\u001B[39m edge_mask\u001B[38;5;241m.\u001B[39mview(batch_size \u001B[38;5;241m*\u001B[39m n_nodes \u001B[38;5;241m*\u001B[39m n_nodes, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m (h, distance, edges, node_mask, edge_mask)\n\u001B[1;32m--> 163\u001B[0m output, distances, edges, node_mask, edge_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLayer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout(output)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m    165\u001B[0m output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mview(batch_size,n_nodes)\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36mHGCLayer.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     70\u001B[0m h, distances, edges, node_mask, edge_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[0;32m     71\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mHypLinear(h)\n\u001B[1;32m---> 72\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHypAgg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistances\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medges\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mHypAct(h)\n\u001B[0;32m     74\u001B[0m output \u001B[38;5;241m=\u001B[39m (h, distances, edges, node_mask, edge_mask)\n",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36mHGCLayer.HypAgg\u001B[1;34m(self, x, distances, edges, node_mask, edge_mask)\u001B[0m\n\u001B[0;32m     92\u001B[0m x_tangent_row \u001B[38;5;241m=\u001B[39m x_tangent[row]\n\u001B[0;32m     93\u001B[0m x_tangent_col \u001B[38;5;241m=\u001B[39m x_tangent[col]\n\u001B[1;32m---> 95\u001B[0m x_local_tangent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanifold\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_in\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (b*n_node*n_node,dim)  x_col落在x_row的切空间\u001B[39;00m\n\u001B[0;32m     97\u001B[0m att \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39matt(x_tangent_row, x_tangent_col, distances,edge_mask)  \u001B[38;5;66;03m# (b*n_node*n_node,dim)\u001B[39;00m\n\u001B[0;32m     99\u001B[0m agg \u001B[38;5;241m=\u001B[39m x_local_tangent \u001B[38;5;241m*\u001B[39m att\n",
      "File \u001B[1;32mD:\\GitHub repo\\Hyperbolic-Diffusion\\manifolds\\hyperboloid.py:129\u001B[0m, in \u001B[0;36mHyperboloid.logmap\u001B[1;34m(self, x, y, c)\u001B[0m\n\u001B[0;32m    127\u001B[0m dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msqdist(x, y, c) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[0;32m    128\u001B[0m result \u001B[38;5;241m=\u001B[39m dist \u001B[38;5;241m*\u001B[39m u \u001B[38;5;241m/\u001B[39m normu\n\u001B[1;32m--> 129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproj_tan\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\GitHub repo\\Hyperbolic-Diffusion\\manifolds\\hyperboloid.py:83\u001B[0m, in \u001B[0;36mHyperboloid.proj_tan\u001B[1;34m(self, u, x, c)\u001B[0m\n\u001B[0;32m     81\u001B[0m vals \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(u)\n\u001B[0;32m     82\u001B[0m vals[:, \u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m ux \u001B[38;5;241m/\u001B[39m torch\u001B[38;5;241m.\u001B[39mclamp(x[:, \u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meps[x\u001B[38;5;241m.\u001B[39mdtype])\n\u001B[1;32m---> 83\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mvals\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mu\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    loss_sum,n,t = 0,0,0.0\n",
    "    counter = 0\n",
    "    KL_sum = 0\n",
    "    for input in (train_loader):\n",
    "        # counter+=1\n",
    "        # if counter>10:\n",
    "        #     sys.exit(0)\n",
    "        for key in input:\n",
    "            input[key] = input[key].to(device)\n",
    "        t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input)\n",
    "        step+=1\n",
    "        print('step',step,' loss:',loss,' lr: ',lr_scheduler.get_last_lr())\n",
    "        # print('KL:',KL)\n",
    "        # curvatures = list(model.get_submodule('encoder.curvatures'))\n",
    "        # print('encoder:',curvatures)\n",
    "        # curvatures = list(model.get_submodule('decoder.curvatures'))\n",
    "        # print('decoder:',curvatures)\n",
    "\n",
    "        loss.backward()\n",
    "        loss_sum += loss\n",
    "        n += 1\n",
    "        # if args.grad_clip is not None:\n",
    "        #     max_norm = float(args.grad_clip)\n",
    "        #     all_params = list(model.parameters())\n",
    "        #     for param in all_params:\n",
    "        #         torch.nn.utils.clip_grad_norm_(param, max_norm)\n",
    "\n",
    "        if args.grad_clip is not None:\n",
    "            grad_clip = float(args.grad_clip)\n",
    "            all_params = list(model.parameters())\n",
    "            for param in all_params:\n",
    "                torch.nn.utils.clip_grad_value_(param, grad_clip)\n",
    "        optimizer.step()\n",
    "        # curvatures = list(model.get_submodule('encoder.curvatures'))\n",
    "        # print('encoder:',curvatures)\n",
    "        # curvatures = list(model.get_submodule('decoder.curvatures'))\n",
    "        # print('decoder:',curvatures)\n",
    "        # en_curvatures = model.get_submodule('encoder.curvatures')\n",
    "        # for p in en_curvatures.parameters():\n",
    "        #     p.data.clamp_(1e-8)\n",
    "        # de_curvatures = model.get_submodule('decoder.curvatures')\n",
    "        # for p in de_curvatures.parameters():\n",
    "        #     p.data.clamp_(1e-8)\n",
    "        lr_scheduler.step()\n",
    "    if (epoch + 1) % args.log_freq == 0:\n",
    "        str = \" \".join(['Epoch: {:04d}'.format(epoch + 1),\n",
    "                               'lr: {}'.format(lr_scheduler.get_last_lr()[0]),\n",
    "                               'loss: {:.4f}'.format(loss_sum/n),\n",
    "                               'time: {:.4f}s'.format(time.time() - t)\n",
    "                               ])\n",
    "        print(str)\n",
    "        # logging.info(str)\n",
    "        # curvatures = list(model.get_submodule('encoder.curvatures'))\n",
    "        # print('encoder:',curvatures)\n",
    "        # curvatures = list(model.get_submodule('decoder.curvatures'))\n",
    "        # print('decoder:',curvatures)\n",
    "\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     loss_sum,n = 0.0,0\n",
    "    #     for input in tqdm(val_loader):\n",
    "    #         for key in input:\n",
    "    #             input[key] = input[key].to(torch.device('cuda'))\n",
    "    #         t = time.time()\n",
    "    #         model.train()\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss,KL = model(input)\n",
    "    #         loss+=KL\n",
    "    #         n += 1\n",
    "    #         loss_sum += loss\n",
    "    #\n",
    "    #     print('val_loss:',loss_sum.item()/n)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "hyperbolid\n",
    "val_loss: tensor(0.0009, device='cuda:0') encoder结束后投影到欧氏空间\n",
    "val_loss: tensor(1.5983e-05, device='cuda:0') encoder结束后保持在双曲空间\n",
    "欧氏空间：\n",
    "val_loss: tensor(0.1687, device='cuda:0')\n",
    "\n",
    "val_loss: tensor(0.2672, device='cuda:0') 坐标在双曲空间 encoder结束后保持在双曲空间\n",
    "val_loss: tensor(0.0075, device='cuda:0') 坐标在双曲空间 encoder结束后投影到欧氏空间\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), './saved_model/'+args.model+'-encoder_kl.pt')\n",
    "torch.save(model.decoder.state_dict(), './saved_model/'+args.model+'-decoder_kl.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}