{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack as spk\n",
    "import os\n",
    "from my_config import config_args\n",
    "from Model.HGDM import HyperbolicAE,HyperbolicDiffusion\n",
    "import optimizers\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 获得训练数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133885\n"
     ]
    }
   ],
   "source": [
    "qm9data = QM9('./data/qm9.db', download=True,load_only=[QM9.U0])\n",
    "qm9split = './data/qm9split'\n",
    "print(len(qm9data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 93885\n"
     ]
    }
   ],
   "source": [
    "train, val, test = spk.train_test_split(\n",
    "        data=qm9data,\n",
    "        num_train=30000,\n",
    "        num_val=10000,\n",
    "        split_file=os.path.join(qm9split, \"split30000-10000.npz\"),\n",
    "    )\n",
    "print(len(train),len(val),len(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_loader = spk.AtomsLoader(train, batch_size=256, shuffle=False)\n",
    "val_loader = spk.AtomsLoader(val, batch_size=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "class obj(object):\n",
    "    def __init__(self, dict_):\n",
    "        self.__dict__.update(dict_)\n",
    "args = json.loads(json.dumps(config_args), object_hook=obj)\n",
    "\n",
    "model = HyperbolicAE(args)\n",
    "\n",
    "optimizer = getattr(optimizers, args.optimizer)(params=model.parameters(), lr=args.lr,\n",
    "                                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=args.lr_reduce_freq,\n",
    "    gamma=float(args.gamma)\n",
    ")\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "logging.info(f\"Total number of parameters: {tot_params}\")\n",
    "device = torch.device('cuda')\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1  loss: tensor(123.1349, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 2  loss: tensor(96.9694, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 3  loss: tensor(84.8839, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 4  loss: tensor(72.3122, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 5  loss: tensor(66.6071, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 6  loss: tensor(66.0879, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 7  loss: tensor(59.4141, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 8  loss: tensor(60.4246, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 9  loss: tensor(50.9489, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 10  loss: tensor(52.1512, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 11  loss: tensor(55.9561, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 12  loss: tensor(54.5654, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 13  loss: tensor(54.2013, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 14  loss: tensor(48.9323, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 15  loss: tensor(52.9701, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 16  loss: tensor(52.4098, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 17  loss: tensor(51.2059, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 18  loss: tensor(50.7210, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 19  loss: tensor(50.1543, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 20  loss: tensor(49.6231, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 21  loss: tensor(43.9623, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 22  loss: tensor(48.1709, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 23  loss: tensor(47.9912, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 24  loss: tensor(47.9417, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 25  loss: tensor(42.2151, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 26  loss: tensor(41.4267, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 27  loss: tensor(41.2262, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 28  loss: tensor(50.0274, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 29  loss: tensor(49.6158, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 30  loss: tensor(43.5058, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 31  loss: tensor(43.9582, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 32  loss: tensor(38.2968, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 33  loss: tensor(43.7416, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 34  loss: tensor(42.8176, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 35  loss: tensor(38.4516, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 36  loss: tensor(42.3304, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 37  loss: tensor(47.2601, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 38  loss: tensor(37.3748, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 39  loss: tensor(37.0638, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 40  loss: tensor(42.5214, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 41  loss: tensor(41.4206, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 42  loss: tensor(38.8747, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 43  loss: tensor(37.3187, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 44  loss: tensor(36.2506, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 45  loss: tensor(40.9620, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 46  loss: tensor(40.3256, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 47  loss: tensor(45.3280, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 48  loss: tensor(35.7780, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 49  loss: tensor(39.7336, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 50  loss: tensor(39.9007, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 51  loss: tensor(40.7187, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 52  loss: tensor(39.4753, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 53  loss: tensor(34.8315, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 54  loss: tensor(34.6567, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 55  loss: tensor(39.7870, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 56  loss: tensor(38.8585, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 57  loss: tensor(35.6546, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 58  loss: tensor(38.3391, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 59  loss: tensor(38.1880, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 60  loss: tensor(37.8206, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 61  loss: tensor(38.4794, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 62  loss: tensor(33.2214, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 63  loss: tensor(33.0269, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 64  loss: tensor(37.7445, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 65  loss: tensor(35.7361, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 66  loss: tensor(33.0872, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 67  loss: tensor(33.2181, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 68  loss: tensor(33.0599, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 69  loss: tensor(32.0915, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 70  loss: tensor(37.0045, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 71  loss: tensor(32.3372, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 72  loss: tensor(36.7296, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 73  loss: tensor(31.5550, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 74  loss: tensor(34.6705, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 75  loss: tensor(36.3762, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 76  loss: tensor(31.2174, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 77  loss: tensor(35.9137, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 78  loss: tensor(31.3847, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 79  loss: tensor(31.4367, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 80  loss: tensor(30.9919, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 81  loss: tensor(35.4227, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 82  loss: tensor(35.2330, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 83  loss: tensor(30.1110, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 84  loss: tensor(34.3514, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 85  loss: tensor(34.3003, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 86  loss: tensor(34.0353, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 87  loss: tensor(29.8808, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 88  loss: tensor(34.1123, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 89  loss: tensor(33.5369, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 90  loss: tensor(29.7339, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 91  loss: tensor(28.9742, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 92  loss: tensor(33.1036, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 93  loss: tensor(28.7230, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 94  loss: tensor(33.4939, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 95  loss: tensor(32.0338, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 96  loss: tensor(33.2975, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 97  loss: tensor(31.9718, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 98  loss: tensor(23.7263, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 99  loss: tensor(29.4127, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 100  loss: tensor(35.6655, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 101  loss: tensor(30.9069, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 102  loss: tensor(25.9594, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 103  loss: tensor(30.5153, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 104  loss: tensor(29.1010, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 105  loss: tensor(33.4936, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 106  loss: tensor(33.1127, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 107  loss: tensor(24.1355, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 108  loss: tensor(27.8574, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 109  loss: tensor(23.7261, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 110  loss: tensor(30.9365, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 111  loss: tensor(25.9609, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 112  loss: tensor(21.1078, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 113  loss: tensor(21.0640, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 114  loss: tensor(21.5410, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 115  loss: tensor(20.6874, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 116  loss: tensor(19.8893, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 117  loss: tensor(23.1337, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 118  loss: tensor(18.4337, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "Epoch: 0001 lr: 0.001 loss: 39.8084 KL:0.1494 time: 0.0515s\n",
      "step 119  loss: tensor(18.5733, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 120  loss: tensor(17.5856, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 121  loss: tensor(17.3973, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 122  loss: tensor(16.7204, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 123  loss: tensor(17.1383, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 124  loss: tensor(20.3882, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 125  loss: tensor(18.9933, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 126  loss: tensor(20.3372, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 127  loss: tensor(13.9196, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 128  loss: tensor(16.4706, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 129  loss: tensor(20.5682, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 130  loss: tensor(18.9232, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 131  loss: tensor(19.4335, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 132  loss: tensor(15.9683, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 133  loss: tensor(19.6754, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 134  loss: tensor(19.6915, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 135  loss: tensor(18.7159, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 136  loss: tensor(18.7526, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 137  loss: tensor(18.7965, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 138  loss: tensor(18.8471, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 139  loss: tensor(14.2260, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 140  loss: tensor(18.5841, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 141  loss: tensor(19.3784, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 142  loss: tensor(19.5888, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 143  loss: tensor(15.3563, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 144  loss: tensor(14.6198, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 145  loss: tensor(14.7844, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 146  loss: tensor(22.7340, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 147  loss: tensor(22.6908, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 148  loss: tensor(17.7484, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 149  loss: tensor(18.1671, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 150  loss: tensor(13.4884, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 151  loss: tensor(18.6694, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 152  loss: tensor(17.8380, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 153  loss: tensor(14.3966, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 154  loss: tensor(17.7711, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 155  loss: tensor(22.1520, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 156  loss: tensor(13.7792, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 157  loss: tensor(13.6495, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 158  loss: tensor(18.4239, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 159  loss: tensor(17.7760, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 160  loss: tensor(16.1700, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 161  loss: tensor(14.8351, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 162  loss: tensor(13.8416, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 163  loss: tensor(18.3040, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 164  loss: tensor(17.7304, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 165  loss: tensor(22.0982, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 166  loss: tensor(14.0870, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 167  loss: tensor(17.4370, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 168  loss: tensor(17.7927, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 169  loss: tensor(18.6686, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 170  loss: tensor(17.6699, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 171  loss: tensor(14.1749, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 172  loss: tensor(13.8101, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 173  loss: tensor(18.1772, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 174  loss: tensor(17.6136, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 175  loss: tensor(15.0711, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 176  loss: tensor(17.1535, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 177  loss: tensor(17.1452, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 178  loss: tensor(16.9916, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 179  loss: tensor(17.7507, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 180  loss: tensor(13.3576, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 181  loss: tensor(13.3378, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 182  loss: tensor(17.3558, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 183  loss: tensor(15.8223, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 184  loss: tensor(13.6510, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 185  loss: tensor(13.8449, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 186  loss: tensor(13.7198, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 187  loss: tensor(13.0074, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 188  loss: tensor(17.3182, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 189  loss: tensor(13.5129, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 190  loss: tensor(17.1408, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 191  loss: tensor(13.0572, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 192  loss: tensor(15.8593, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 193  loss: tensor(17.4573, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 194  loss: tensor(13.0813, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 195  loss: tensor(17.2757, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 196  loss: tensor(13.4846, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 197  loss: tensor(13.7687, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 198  loss: tensor(13.2623, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 199  loss: tensor(17.3814, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 200  loss: tensor(17.0884, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 201  loss: tensor(12.9572, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 202  loss: tensor(16.4644, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 203  loss: tensor(16.7059, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 204  loss: tensor(16.6883, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 205  loss: tensor(13.3061, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 206  loss: tensor(16.9223, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 207  loss: tensor(16.3252, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n",
      "step 208  loss: tensor(13.5120, device='cuda:0', grad_fn=<DivBackward0>)  lr:  [0.001]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# print('KL:',KL)\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# curvatures = list(model.get_submodule('encoder.curvatures'))\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# print('encoder:',curvatures)\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# curvatures = list(model.get_submodule('decoder.curvatures'))\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# print('decoder:',curvatures)\u001B[39;00m\n\u001B[0;32m     25\u001B[0m loss\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39mKL\n\u001B[1;32m---> 26\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m loss_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[0;32m     28\u001B[0m KL_sum\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39mKL\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    loss_sum,n,t = 0,0,0.0\n",
    "    counter = 0\n",
    "    KL_sum = 0\n",
    "    for input in (train_loader):\n",
    "        # counter+=1\n",
    "        # if counter>10:\n",
    "        #     sys.exit(0)\n",
    "        for key in input:\n",
    "            input[key] = input[key].to(torch.device('cuda'))\n",
    "        t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss,KL = model(input)\n",
    "        step+=1\n",
    "        print('step',step,' loss:',loss,' lr: ',lr_scheduler.get_last_lr())\n",
    "        # print('KL:',KL)\n",
    "        # curvatures = list(model.get_submodule('encoder.curvatures'))\n",
    "        # print('encoder:',curvatures)\n",
    "        # curvatures = list(model.get_submodule('decoder.curvatures'))\n",
    "        # print('decoder:',curvatures)\n",
    "\n",
    "        loss+=KL\n",
    "        loss.backward()\n",
    "        loss_sum += loss\n",
    "        KL_sum+=KL\n",
    "        n += 1\n",
    "        # if args.grad_clip is not None:\n",
    "        #     max_norm = float(args.grad_clip)\n",
    "        #     all_params = list(model.parameters())\n",
    "        #     for param in all_params:\n",
    "        #         torch.nn.utils.clip_grad_norm_(param, max_norm)\n",
    "\n",
    "        if args.grad_clip is not None:\n",
    "            grad_clip = float(args.grad_clip)\n",
    "            all_params = list(model.parameters())\n",
    "            for param in all_params:\n",
    "                torch.nn.utils.clip_grad_value_(param, grad_clip)\n",
    "        optimizer.step()\n",
    "        en_curvatures = model.get_submodule('encoder.curvatures')\n",
    "        for p in en_curvatures.parameters():\n",
    "            p.data.clamp_(1e-8)\n",
    "        de_curvatures = model.get_submodule('decoder.curvatures')\n",
    "        for p in de_curvatures.parameters():\n",
    "            p.data.clamp_(1e-8)\n",
    "        lr_scheduler.step()\n",
    "    if (epoch + 1) % args.log_freq == 0:\n",
    "        str = \" \".join(['Epoch: {:04d}'.format(epoch + 1),\n",
    "                               'lr: {}'.format(lr_scheduler.get_last_lr()[0]),\n",
    "                               'loss: {:.4f}'.format(loss_sum/n),\n",
    "                               'KL:{:.4f}'.format(KL_sum/n),\n",
    "                               'time: {:.4f}s'.format(time.time() - t)\n",
    "                               ])\n",
    "        print(str)\n",
    "        # logging.info(str)\n",
    "        # curvatures = list(model.get_submodule('encoder.curvatures'))\n",
    "        # print('encoder:',curvatures)\n",
    "        # curvatures = list(model.get_submodule('decoder.curvatures'))\n",
    "        # print('decoder:',curvatures)\n",
    "\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     loss_sum,n = 0.0,0\n",
    "    #     for input in tqdm(val_loader):\n",
    "    #         for key in input:\n",
    "    #             input[key] = input[key].to(torch.device('cuda'))\n",
    "    #         t = time.time()\n",
    "    #         model.train()\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss,KL = model(input)\n",
    "    #         loss+=KL\n",
    "    #         n += 1\n",
    "    #         loss_sum += loss\n",
    "    #\n",
    "    #     print('val_loss:',loss_sum.item()/n)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "hyperbolid\n",
    "val_loss: tensor(0.0009, device='cuda:0') encoder结束后投影到欧氏空间\n",
    "val_loss: tensor(1.5983e-05, device='cuda:0') encoder结束后保持在双曲空间\n",
    "欧氏空间：\n",
    "val_loss: tensor(0.1687, device='cuda:0')\n",
    "\n",
    "val_loss: tensor(0.2672, device='cuda:0') 坐标在双曲空间 encoder结束后保持在双曲空间\n",
    "val_loss: tensor(0.0075, device='cuda:0') 坐标在双曲空间 encoder结束后投影到欧氏空间\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), './saved_model/'+args.model+'-encoder_kl.pt')\n",
    "torch.save(model.decoder.state_dict(), './saved_model/'+args.model+'-decoder_kl.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}