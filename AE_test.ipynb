{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack as spk\n",
    "import os\n",
    "from my_config import config_args\n",
    "from Model.HGDM import HyperbolicAE,HyperbolicDiffusion\n",
    "import optimizers\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 获得训练数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133885\n"
     ]
    }
   ],
   "source": [
    "qm9data = QM9('./data/qm9.db', download=True,load_only=[QM9.U0])\n",
    "qm9split = './data/qm9split'\n",
    "print(len(qm9data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 93885\n"
     ]
    }
   ],
   "source": [
    "train, val, test = spk.train_test_split(\n",
    "        data=qm9data,\n",
    "        num_train=30000,\n",
    "        num_val=10000,\n",
    "        split_file=os.path.join(qm9split, \"split30000-10000.npz\"),\n",
    "    )\n",
    "print(len(train),len(val),len(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_loader = spk.AtomsLoader(train, batch_size=256, shuffle=False)\n",
    "val_loader = spk.AtomsLoader(val, batch_size=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "class obj(object):\n",
    "    def __init__(self, dict_):\n",
    "        self.__dict__.update(dict_)\n",
    "args = json.loads(json.dumps(config_args), object_hook=obj)\n",
    "\n",
    "model = HyperbolicAE(args)\n",
    "\n",
    "optimizer = getattr(optimizers, args.optimizer)(params=model.parameters(), lr=args.lr,\n",
    "                                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=args.lr_reduce_freq,\n",
    "    gamma=float(args.gamma)\n",
    ")\n",
    "tot_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "logging.info(f\"Total number of parameters: {tot_params}\")\n",
    "device = torch.device('cuda')\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/118 [00:02<04:08,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(181.5335, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/118 [00:02<02:12,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(184.4654, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/118 [00:03<01:35,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(185.2517, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/118 [00:03<01:18,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(182.2665, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/118 [00:03<01:07,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(184.0785, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/118 [00:04<01:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(190.9750, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 7/118 [00:04<00:58,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(184.0560, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 8/118 [00:05<00:55,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(188.9900, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 9/118 [00:05<00:52,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(177.8144, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10/118 [00:06<00:51,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(182.3669, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/118 [00:06<00:50,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(191.2399, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/118 [00:07<01:09,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(189.8099, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# print('KL:',KL)\u001B[39;00m\n\u001B[0;32m     18\u001B[0m loss\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39mKL\n\u001B[1;32m---> 19\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m loss_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[0;32m     21\u001B[0m KL_sum\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39mKL\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\hdmm\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    loss_sum,n,t = 0,0,0.0\n",
    "    counter = 0\n",
    "    KL_sum = 0\n",
    "    for input in tqdm(train_loader):\n",
    "        # counter+=1\n",
    "        # if counter>10:\n",
    "        #     sys.exit(0)\n",
    "        for key in input:\n",
    "            input[key] = input[key].to(torch.device('cuda'))\n",
    "        t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss,KL = model(input)\n",
    "        print('loss:',loss)\n",
    "        # print('KL:',KL)\n",
    "\n",
    "        loss+=KL\n",
    "        loss.backward()\n",
    "        loss_sum += loss\n",
    "        KL_sum+=KL\n",
    "        n += 1\n",
    "        # if args.grad_clip is not None:\n",
    "        #     max_norm = float(args.grad_clip)\n",
    "        #     all_params = list(model.parameters())\n",
    "        #     for param in all_params:\n",
    "        #         torch.nn.utils.clip_grad_norm_(param, max_norm)\n",
    "\n",
    "        if args.grad_clip is not None:\n",
    "            grad_clip = float(args.grad_clip)\n",
    "            all_params = list(model.parameters())\n",
    "            for param in all_params:\n",
    "                torch.nn.utils.clip_grad_value_(param, grad_clip)\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    if (epoch + 1) % args.log_freq == 0:\n",
    "        str = \" \".join(['Epoch: {:04d}'.format(epoch + 1),\n",
    "                               'lr: {}'.format(lr_scheduler.get_last_lr()[0]),\n",
    "                               'loss: {:.4f}'.format(loss_sum/n),\n",
    "                               'KL:{:.4f}'.format(KL_sum/n),\n",
    "                               'time: {:.4f}s'.format(time.time() - t)\n",
    "                               ])\n",
    "        print(str)\n",
    "        # logging.info(str)\n",
    "        # curvatures = list(model.get_submodule('encoder.curvatures'))\n",
    "        # print('encoder:',curvatures)\n",
    "        # curvatures = list(model.get_submodule('decoder.curvatures'))\n",
    "        # print('decoder:',curvatures)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum,n = 0.0,0\n",
    "        for input in tqdm(val_loader):\n",
    "            for key in input:\n",
    "                input[key] = input[key].to(torch.device('cuda'))\n",
    "            t = time.time()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss,KL = model(input)\n",
    "            loss+=KL\n",
    "            n += 1\n",
    "            loss_sum += loss\n",
    "\n",
    "        print('val_loss:',loss_sum.item()/n)\n",
    "\n",
    "\"\"\"\n",
    "Epoch: 0017 lr: 0.008 loss: 1.1283 KL:0.1778 time: 0.0862s\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "hyperbolid\n",
    "val_loss: tensor(0.0009, device='cuda:0') encoder结束后投影到欧氏空间\n",
    "val_loss: tensor(1.5983e-05, device='cuda:0') encoder结束后保持在双曲空间\n",
    "欧氏空间：\n",
    "val_loss: tensor(0.1687, device='cuda:0')\n",
    "\n",
    "val_loss: tensor(0.2672, device='cuda:0') 坐标在双曲空间 encoder结束后保持在双曲空间\n",
    "val_loss: tensor(0.0075, device='cuda:0') 坐标在双曲空间 encoder结束后投影到欧氏空间\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), './saved_model/'+args.model+'-encoder_kl.pt')\n",
    "torch.save(model.decoder.state_dict(), './saved_model/'+args.model+'-decoder_kl.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}